{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Inference on an upcoming dataset\n",
    "\n",
    "In this part, we will simulate the real deployment of the package and make inference on an upcoming dataset.\n",
    "\n",
    "## Training models\n",
    "\n",
    "Similar to the first example, we initialize a `Trainer` and model bases."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Project will be saved to ../../../../output/sample/2023-07-29-17-20-51-0_sample\n",
      "Dataset size: 153 51 52\n",
      "Data saved to ../../../../output/sample/2023-07-29-17-20-51-0_sample (data.csv and tabular_data.csv).\n",
      "/home/xlluo/anaconda3/envs/mlfatigue/lib/python3.8/site-packages/pytorch_lightning/utilities/cloud_io.py:33: LightningDeprecationWarning: `pytorch_lightning.utilities.cloud_io.get_filesystem` has been deprecated in v1.8.0 and will be removed in v1.10.0. Please use `lightning_lite.utilities.cloud_io.get_filesystem` instead.\n",
      "  rank_zero_deprecation(\n",
      "Trainer saved. To load the trainer, run trainer = load_trainer(path='../../../../output/sample/2023-07-29-17-20-51-0_sample/trainer.pkl')\n",
      "Trainer saved. To load the trainer, run trainer = load_trainer(path='../../../../output/sample/2023-07-29-17-20-51-0_sample/trainer.pkl')\n",
      "Trainer saved. To load the trainer, run trainer = load_trainer(path='../../../../output/sample/2023-07-29-17-20-51-0_sample/trainer.pkl')\n",
      "PytorchTabular metrics\n",
      "Category Embedding 1/1\n",
      "WideDeep metrics\n",
      "TabMlp 1/1\n",
      "AutoGluon metrics\n",
      "Linear Regression 1/1\n",
      "Trainer saved. To load the trainer, run trainer = load_trainer(path='../../../../output/sample/2023-07-29-17-20-51-0_sample/trainer.pkl')\n"
     ]
    },
    {
     "data": {
      "text/plain": "          Program               Model  Training RMSE  Training MSE  \\\n0        WideDeep              TabMlp     117.866783  13892.578569   \n1       AutoGluon   Linear Regression     114.065981  13011.048090   \n2  PytorchTabular  Category Embedding     103.473606  10706.787126   \n\n   Training MAE  Training MAPE  Training R2  Training RMSE_CONSERV  \\\n0     95.954615       1.767595     0.578265           12843.832490   \n1     91.398514       2.686924     0.605025           12364.215702   \n2     85.594475       2.112976     0.674976           10549.942827   \n\n   Testing RMSE   Testing MSE  Testing MAE  Testing MAPE  Testing R2  \\\n0    136.008612  18498.342515   108.973634      3.452354    0.375839   \n1    139.269733  19396.058516   119.072766      4.078846    0.345548   \n2    143.043941  20461.568975   121.271502      4.315138    0.309596   \n\n   Testing RMSE_CONSERV  Validation RMSE  Validation MSE  Validation MAE  \\\n0          11574.749350       103.435861    10698.977274       84.573629   \n1          11994.905029       110.253538    12155.842646       88.607594   \n2          13106.784041       106.790754    11404.265107       83.616240   \n\n   Validation MAPE  Validation R2  Validation RMSE_CONSERV  \n0         1.402757       0.516810             12147.943417  \n1         1.546470       0.451015             12189.700803  \n2         1.295624       0.484958             13734.326173  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Program</th>\n      <th>Model</th>\n      <th>Training RMSE</th>\n      <th>Training MSE</th>\n      <th>Training MAE</th>\n      <th>Training MAPE</th>\n      <th>Training R2</th>\n      <th>Training RMSE_CONSERV</th>\n      <th>Testing RMSE</th>\n      <th>Testing MSE</th>\n      <th>Testing MAE</th>\n      <th>Testing MAPE</th>\n      <th>Testing R2</th>\n      <th>Testing RMSE_CONSERV</th>\n      <th>Validation RMSE</th>\n      <th>Validation MSE</th>\n      <th>Validation MAE</th>\n      <th>Validation MAPE</th>\n      <th>Validation R2</th>\n      <th>Validation RMSE_CONSERV</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>WideDeep</td>\n      <td>TabMlp</td>\n      <td>117.866783</td>\n      <td>13892.578569</td>\n      <td>95.954615</td>\n      <td>1.767595</td>\n      <td>0.578265</td>\n      <td>12843.832490</td>\n      <td>136.008612</td>\n      <td>18498.342515</td>\n      <td>108.973634</td>\n      <td>3.452354</td>\n      <td>0.375839</td>\n      <td>11574.749350</td>\n      <td>103.435861</td>\n      <td>10698.977274</td>\n      <td>84.573629</td>\n      <td>1.402757</td>\n      <td>0.516810</td>\n      <td>12147.943417</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>AutoGluon</td>\n      <td>Linear Regression</td>\n      <td>114.065981</td>\n      <td>13011.048090</td>\n      <td>91.398514</td>\n      <td>2.686924</td>\n      <td>0.605025</td>\n      <td>12364.215702</td>\n      <td>139.269733</td>\n      <td>19396.058516</td>\n      <td>119.072766</td>\n      <td>4.078846</td>\n      <td>0.345548</td>\n      <td>11994.905029</td>\n      <td>110.253538</td>\n      <td>12155.842646</td>\n      <td>88.607594</td>\n      <td>1.546470</td>\n      <td>0.451015</td>\n      <td>12189.700803</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>PytorchTabular</td>\n      <td>Category Embedding</td>\n      <td>103.473606</td>\n      <td>10706.787126</td>\n      <td>85.594475</td>\n      <td>2.112976</td>\n      <td>0.674976</td>\n      <td>10549.942827</td>\n      <td>143.043941</td>\n      <td>20461.568975</td>\n      <td>121.271502</td>\n      <td>4.315138</td>\n      <td>0.309596</td>\n      <td>13106.784041</td>\n      <td>106.790754</td>\n      <td>11404.265107</td>\n      <td>83.616240</td>\n      <td>1.295624</td>\n      <td>0.484958</td>\n      <td>13734.326173</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from tabensemb.trainer import Trainer\n",
    "from tabensemb.model import *\n",
    "import tabensemb\n",
    "import os\n",
    "\n",
    "prefix = \"../../../../\"\n",
    "tabensemb.setting[\"default_output_path\"] = prefix + \"output\"\n",
    "tabensemb.setting[\"default_config_path\"] = prefix + \"configs\"\n",
    "tabensemb.setting[\"default_data_path\"] = prefix + \"data\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using {} device\".format(device))\n",
    "\n",
    "trainer = Trainer(device=device)\n",
    "trainer.load_config(\"sample\")\n",
    "\n",
    "from tabensemb.utils import Logging\n",
    "log = Logging()\n",
    "log.enter(os.path.join(trainer.project_root, \"log.txt\"))\n",
    "\n",
    "trainer.load_data()\n",
    "\n",
    "models = [\n",
    "    PytorchTabular(trainer, model_subset=[\"Category Embedding\"]),\n",
    "    WideDeep(trainer, model_subset=[\"TabMlp\"]),\n",
    "    AutoGluon(trainer, model_subset=[\"Linear Regression\"]),\n",
    "]\n",
    "trainer.add_modelbases(models)\n",
    "\n",
    "trainer.train(verbose=False, stderr_to_stdout=True)\n",
    "trainer.get_leaderboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "*Optional*: Use the following line, we can run multiple random trials based on different random seeds and take the average of metrics to evaluate models.\n",
    "\n",
    "```python\n",
    "trainer.get_leaderboard(cross_validation=2, split_type=\"random\", stderr_to_stdout=True)\n",
    "```\n",
    "\n",
    "**Remark**: `split_type` can be `cv`, which represents k-fold cross-validation where k is `cross_validation`. Here `split_type=\"random\"` means that the dataset is randomly split according to the given `split_ratio` in the configuration and different random seeds."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Selecting and storing a model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "From the leaderboard, we can check the performance of each model and select one of the models for deployment. Say we want to choose `TabMlp` from `WideDeep` (`pytorch_widedeep`), we detach the model from the heavy `trainer`. It is also stored locally in a separate directory."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer saved. To load the trainer, run trainer = load_trainer(path='../../../../output/sample/2023-07-29-17-20-51-0_sample-I1/trainer.pkl')\n"
     ]
    }
   ],
   "source": [
    "trainer_of_one_model = trainer.detach_model(program=\"WideDeep\", model_name=\"TabMlp\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The detached trainer now has only one model base."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "[<tabensemb.model.widedeep.WideDeep at 0x7f63607f9880>]"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model bases of the detached trainer\n",
    "trainer_of_one_model.modelbases"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "<pytorch_widedeep.training.trainer.Trainer at 0x7f627690f130>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The model in the model base\n",
    "trainer_of_one_model.get_modelbase(\"WideDeep_TabMlp\").model[\"TabMlp\"]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loading the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now the `Trainer` containing a single model stores in a seperate directory. Assume that we want to load the local trainer in a separate script for inference. In the following line, the argument `path` of `load_trainer` is the path to `trainer.pkl`, which is already printed when detaching the model or training the model bases. Here we just use the directory of the detached trainer `trainer_of_one_model`.\n",
    "\n",
    "**Remark**: You can move the directory to any other places (or other devices if the version of the package and the environment are all consistent) and rename the folder. `tabensemb` automatically configures the path."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from tabensemb.trainer import load_trainer\n",
    "\n",
    "trainer = load_trainer(path=os.path.join(trainer_of_one_model.project_root, \"trainer.pkl\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "<pytorch_widedeep.training.trainer.Trainer at 0x7f63608885e0>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.get_modelbase(\"WideDeep_TabMlp\").model[\"TabMlp\"]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Inference\n",
    "\n",
    "Assume that we have a new `DataFrame` representing an upcoming dataset. For demonstration, we use the testing set here."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "df = trainer.df.loc[trainer.test_indices, :]\n",
    "truth = trainer.df.loc[trainer.test_indices, trainer.label_name].values.flatten()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Use the functionality of the model base to do inference. You can see the RMSE error on the \"new\" (testing) dataset is the same as that in the above leaderboard."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "136.00861191638165"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tabensemb.utils import metric_sklearn\n",
    "\n",
    "result = trainer.get_modelbase(\"WideDeep_TabMlp\").predict(df, model_name=\"TabMlp\")\n",
    "metric_sklearn(truth, result, \"rmse\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ -74.43971  ],\n       [ -81.44765  ],\n       [ -31.800875 ],\n       [ -75.2285   ],\n       [ -53.677227 ],\n       [  70.04292  ],\n       [ -25.622992 ],\n       [  36.68125  ],\n       [-211.98978  ],\n       [ 159.79549  ],\n       [-102.89287  ],\n       [-107.03044  ],\n       [ -23.381763 ],\n       [  26.2937   ],\n       [ -56.9037   ],\n       [  42.62137  ],\n       [ -79.51051  ],\n       [ -80.04006  ],\n       [ 168.29333  ],\n       [  10.588863 ],\n       [-124.99167  ],\n       [-175.97296  ],\n       [ -52.490498 ],\n       [ 149.38387  ],\n       [ 153.97131  ],\n       [  88.51623  ],\n       [  16.419373 ],\n       [-181.93864  ],\n       [-143.57932  ],\n       [-108.806404 ],\n       [   2.2212856],\n       [ 118.65542  ],\n       [-147.20416  ],\n       [-209.2984   ],\n       [ 158.00734  ],\n       [-126.64957  ],\n       [-173.66629  ],\n       [  -6.8193183],\n       [  38.248684 ],\n       [-183.28394  ],\n       [  70.60948  ],\n       [ 115.3542   ],\n       [-161.44592  ],\n       [ 135.02829  ],\n       [  15.535942 ],\n       [ 143.6388   ],\n       [ 121.72353  ],\n       [  33.25468  ],\n       [-157.98293  ],\n       [ 147.08133  ],\n       [ 109.99553  ],\n       [  51.764153 ]], dtype=float32)"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}