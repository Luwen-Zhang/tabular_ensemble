{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Customized model base\n",
    "\n",
    "For researchers or model base developers, the basic need is comparing their own models with existing benchmarks in `tabensemb`. In this part, a model base is built within the framework assuming that we want to integrate `TabNet` ([from dreamquark-ai team](https://github.com/dreamquark-ai/tabnet)) into `tabensemb` (indeed `pytorch_tabular` and `pytorch_widedeep` have done that) for regression tasks.\n",
    "\n",
    "**Remark**: For `PyTorch`-based models, we have implemented most requirements of the framework so that users can integrate `torch.nn.Module`s more conveniently. Check \"Customized `PyTorch`-based model base\" for details."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Example: Implement TabNet as a model base"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import tabensemb\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "prefix = \"../../../../\"\n",
    "tabensemb.setting[\"default_output_path\"] = prefix + \"output\"\n",
    "tabensemb.setting[\"default_config_path\"] = prefix + \"configs\"\n",
    "tabensemb.setting[\"default_data_path\"] = prefix + \"data\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "All model bases inherit `AbstractModel` and implement methods within the class. If necessary methods are not implemented, `NotImplementedError` will be raised during usage."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from tabensemb.model import AbstractModel"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We use `scikit-optimize` (https://github.com/scikit-optimize/scikit-optimize) to do Bayesian hyperparameter optimization, so space classes are imported."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from skopt.space import Integer, Real, Categorical"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "First, we define the initialization of the model base. Always remember to pass all args and kwargs to ``__init__`` of `AbstractModel`. Also, we are not discussing classification tasks here, but they are straight forward.\n",
    "\n",
    "```python\n",
    "class TabNet(AbstractModel):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(TabNet, self).__init__(*args, **kwargs)\n",
    "        if self.trainer.datamodule.task != \"regression\":\n",
    "            raise Exception(f\"We only discuss regression tasks here.\")\n",
    "```\n",
    "\n",
    "We should define the name of the model base, and all available models in the model base.\n",
    "\n",
    "```python\n",
    "    def _get_program_name(self):\n",
    "        return \"TabNet\"\n",
    "\n",
    "    def _get_model_names(self):\n",
    "        return [\"TabNet\"]\n",
    "```\n",
    "\n",
    "For each model in the model base, the program will request initial hyperparameters of the model and their search spaces. They are defined as\n",
    "\n",
    "```python\n",
    "    def _space(self, model_name):\n",
    "        return [\n",
    "            Integer(low=4, high=64, prior=\"uniform\", name=\"n_d\", dtype=int),  # 8\n",
    "            Integer(low=4, high=64, prior=\"uniform\", name=\"n_a\", dtype=int),  # 8\n",
    "            Integer(low=3, high=10, prior=\"uniform\", name=\"n_steps\", dtype=int),  # 3\n",
    "            Real(low=1.0, high=2.0, prior=\"uniform\", name=\"gamma\"),  # 1.3\n",
    "            Integer(\n",
    "                low=1, high=5, prior=\"uniform\", name=\"n_independent\", dtype=int\n",
    "            ),  # 2\n",
    "            Integer(low=1, high=5, prior=\"uniform\", name=\"n_shared\", dtype=int),  # 2\n",
    "        ] + self.trainer.SPACE\n",
    "\n",
    "    def _initial_values(self, model_name):\n",
    "        return {\n",
    "            \"n_d\": 8,\n",
    "            \"n_a\": 8,\n",
    "            \"n_steps\": 3,\n",
    "            \"gamma\": 1.3,\n",
    "            \"n_independent\": 2,\n",
    "            \"n_shared\": 2,\n",
    "            \"lr\": self.trainer.args[\"lr\"],\n",
    "            \"weight_decay\": self.trainer.args[\"weight_decay\"],\n",
    "            \"batch_size\": self.trainer.args[\"batch_size\"],\n",
    "        }\n",
    "```\n",
    "\n",
    "Before training, each model base has its own way to process the dataset. Since we can not access the testing set in the training stage, two separate methods are defined to process the whole dataset.\n",
    "\n",
    "`_train_data_preprocess` will return the processed dataset according to a given `Trainer`, which provide all training information and data required. In this example, `X_train/X_val/X_test` represent training/validation/testing sets, and `y_train/y_val/y_test` represent corresponding labels.\n",
    "\n",
    "```python\n",
    "    def _train_data_preprocess(self, model_name):\n",
    "        data = self.trainer.datamodule\n",
    "        cont_feature_names = self.trainer.cont_feature_names\n",
    "        X_train = data.X_train[cont_feature_names].values.astype(np.float32)\n",
    "        X_val = data.X_val[cont_feature_names].values.astype(np.float32)\n",
    "        X_test = data.X_test[cont_feature_names].values.astype(np.float32)\n",
    "        y_train = data.y_train.astype(np.float32)\n",
    "        y_val = data.y_val.astype(np.float32)\n",
    "        y_test = data.y_test.astype(np.float32)\n",
    "\n",
    "        return {\n",
    "            \"X_train\": X_train,\n",
    "            \"y_train\": y_train,\n",
    "            \"X_val\": X_val,\n",
    "            \"y_val\": y_val,\n",
    "            \"X_test\": X_test,\n",
    "            \"y_test\": y_test,\n",
    "        }\n",
    "```\n",
    "\n",
    "Correspondingly, `_data_preprocess` will process an upcoming new dataset, including tabular data `df` containing continuous features and categorical features, and unstacked derived data `derived_data` (multi-modal data or something else depending on the configuration introduced in \"Using data functionalities\"). The returned value should have the same structure as the `X_test` returned in `_train_data_preprocess`.\n",
    "\n",
    "```python\n",
    "    def _data_preprocess(self, df, derived_data, model_name):\n",
    "        return df[self.trainer.cont_feature_names].values.astype(np.float32)\n",
    "```\n",
    "\n",
    "**Remark**: The tabular dataset has gone through all processing stages defined in the `DataModule` inside the trainer **except scaling**. Call `self.trainer.datamodule.data_transform(df, scaler_only=True)` to scale it using the trained scaler if no scaling stage is defined in the model base.\n",
    "\n",
    "The program will pass a selected set of hyperparameters as `kwargs` to initialize a model, train a model, and predict using the model. The returned `model` will be stored locally and reloaded for evaluation and inference, so make sure it contains all information needed to make predictions.\n",
    "\n",
    "```python\n",
    "    def _new_model(self, model_name, verbose, **kwargs):\n",
    "        from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "\n",
    "        def extract_params(**kwargs):\n",
    "            params = {}\n",
    "            optim_params = {}\n",
    "            batch_size = 32\n",
    "            for key, value in kwargs.items():\n",
    "                if key in [\n",
    "                    \"n_d\",\n",
    "                    \"n_a\",\n",
    "                    \"n_steps\",\n",
    "                    \"gamma\",\n",
    "                    \"n_independent\",\n",
    "                    \"n_shared\",\n",
    "                ]:\n",
    "                    params[key] = value\n",
    "                elif key == \"batch_size\":\n",
    "                    batch_size = int(value)\n",
    "                else:\n",
    "                    optim_params[key] = value\n",
    "            return params, optim_params, batch_size\n",
    "\n",
    "        params, optim_params, batch_size = extract_params(**kwargs)\n",
    "\n",
    "        model = TabNetRegressor(\n",
    "            verbose=20 if verbose else 0, optimizer_params=optim_params\n",
    "        )\n",
    "\n",
    "        model.set_params(**params)\n",
    "        return model\n",
    "```\n",
    "\n",
    "**Remark**: `kwargs` has all keys defined in `_initial_values`. If a parameter named `batch_size` is included, a new key named `original_batch_size` exists in `kwargs`. The values of `batch_size` and `original_batch_size` may be different if the program finds that the batch size will make the mini-batches tiny. The threshold is defined by `self.limit_batch_size` (default to 6). A tiny batch might interrupt some models, so it is better to use the modified `batch_size` value.\n",
    "\n",
    "The framework will pass `X_train`, `y_train`, `X_val`, `y_val` from `_train_data_preprocess` to the following `_train_single_model` method, along with some other arguments stating the current training stage. `epoch` is the number of epochs to train the model. `warm_start=True` means the passed model is already trained and should be fine-tuned based on a new dataset. `in_bayes_opt=True` means that the passed `kwargs` is selected by a bayesian hyperparameter optimization step, and a simplified training routine is needed to reduce optimization time.\n",
    "\n",
    "```python\n",
    "    def _train_single_model(\n",
    "        self,\n",
    "        model,\n",
    "        epoch,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        X_val,\n",
    "        y_val,\n",
    "        verbose,\n",
    "        warm_start,\n",
    "        in_bayes_opt,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        eval_set = [(X_val, y_val)]\n",
    "\n",
    "        model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            eval_set=eval_set,\n",
    "            max_epochs=epoch if not in_bayes_opt else self.trainer.args[\"bayes_epoch\"],\n",
    "            patience=self.trainer.args[\"patience\"],\n",
    "            loss_fn=torch.nn.MSELoss(),\n",
    "            eval_metric=[\"mse\"],\n",
    "            batch_size=int(kwargs[\"batch_size\"]),\n",
    "            warm_start=warm_start,\n",
    "            drop_last=False,\n",
    "        )\n",
    "```\n",
    "\n",
    "To evaluate the model or make use of the model, `_pred_single_model` is defined and `X_test` processed in `_train_data_preprocess` or `_data_preprocess` is passed as an argument.\n",
    "\n",
    "```python\n",
    "    def _pred_single_model(self, model, X_test, verbose, **kwargs):\n",
    "        return model.predict(X_test).reshape(-1, 1)\n",
    "```\n",
    "\n",
    "The full code is as followed:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class TabNet(AbstractModel):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(TabNet, self).__init__(*args, **kwargs)\n",
    "        if self.trainer.datamodule.task != \"regression\":\n",
    "            raise Exception(f\"We only discuss regression tasks here.\")\n",
    "\n",
    "    def _get_program_name(self):\n",
    "        return \"TabNet\"\n",
    "\n",
    "    def _get_model_names(self):\n",
    "        return [\"TabNet\"]\n",
    "\n",
    "    def _space(self, model_name):\n",
    "        return [\n",
    "                   Integer(low=4, high=16, prior=\"uniform\", name=\"n_d\", dtype=int),  # 8\n",
    "                   Integer(low=4, high=16, prior=\"uniform\", name=\"n_a\", dtype=int),  # 8\n",
    "                   Integer(low=1, high=6, prior=\"uniform\", name=\"n_steps\", dtype=int),  # 3\n",
    "                   Real(low=1.0, high=1.5, prior=\"uniform\", name=\"gamma\"),  # 1.3\n",
    "                   Integer(\n",
    "                       low=1, high=4, prior=\"uniform\", name=\"n_independent\", dtype=int\n",
    "                   ),  # 2\n",
    "                   Integer(low=1, high=4, prior=\"uniform\", name=\"n_shared\", dtype=int),  # 2\n",
    "               ] + self.trainer.SPACE\n",
    "\n",
    "    def _initial_values(self, model_name):\n",
    "        return {\n",
    "            \"n_d\": 8,\n",
    "            \"n_a\": 8,\n",
    "            \"n_steps\": 3,\n",
    "            \"gamma\": 1.3,\n",
    "            \"n_independent\": 2,\n",
    "            \"n_shared\": 2,\n",
    "            \"lr\": self.trainer.args[\"lr\"],\n",
    "            \"weight_decay\": self.trainer.args[\"weight_decay\"],\n",
    "            \"batch_size\": self.trainer.args[\"batch_size\"],\n",
    "        }\n",
    "\n",
    "    def _train_data_preprocess(self, model_name):\n",
    "        data = self.trainer.datamodule\n",
    "        cont_feature_names = self.trainer.cont_feature_names\n",
    "        X_train = data.X_train[cont_feature_names].values.astype(np.float32)\n",
    "        X_val = data.X_val[cont_feature_names].values.astype(np.float32)\n",
    "        X_test = data.X_test[cont_feature_names].values.astype(np.float32)\n",
    "        y_train = data.y_train.astype(np.float32)\n",
    "        y_val = data.y_val.astype(np.float32)\n",
    "        y_test = data.y_test.astype(np.float32)\n",
    "\n",
    "        return {\n",
    "            \"X_train\": X_train,\n",
    "            \"y_train\": y_train,\n",
    "            \"X_val\": X_val,\n",
    "            \"y_val\": y_val,\n",
    "            \"X_test\": X_test,\n",
    "            \"y_test\": y_test,\n",
    "        }\n",
    "\n",
    "    def _data_preprocess(self, df, derived_data, model_name):\n",
    "        return df[self.trainer.cont_feature_names].values.astype(np.float32)\n",
    "\n",
    "    def _new_model(self, model_name, verbose, **kwargs):\n",
    "        from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "\n",
    "        TabNetRegressor.device_name = \"cpu\"\n",
    "        model = TabNetRegressor(\n",
    "            verbose=20 if verbose else 0, optimizer_params={\"lr\": kwargs[\"lr\"], \"weight_decay\": kwargs[\"weight_decay\"]}\n",
    "        )\n",
    "\n",
    "        model.set_params(\n",
    "            **{\"n_d\": kwargs[\"n_d\"], \"n_a\": kwargs[\"n_a\"], \"n_steps\": kwargs[\"n_steps\"], \"gamma\": kwargs[\"gamma\"],\n",
    "               \"n_independent\": kwargs[\"n_independent\"], \"n_shared\": kwargs[\"n_shared\"]})\n",
    "        return model\n",
    "\n",
    "    def _train_single_model(\n",
    "            self,\n",
    "            model,\n",
    "            epoch,\n",
    "            X_train,\n",
    "            y_train,\n",
    "            X_val,\n",
    "            y_val,\n",
    "            verbose,\n",
    "            warm_start,\n",
    "            in_bayes_opt,\n",
    "            **kwargs,\n",
    "    ):\n",
    "        eval_set = [(X_val, y_val)]\n",
    "\n",
    "        model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            eval_set=eval_set,\n",
    "            max_epochs=epoch if not in_bayes_opt else self.trainer.args[\"bayes_epoch\"],\n",
    "            patience=self.trainer.args[\"patience\"],\n",
    "            loss_fn=torch.nn.MSELoss(),\n",
    "            eval_metric=[\"mse\"],\n",
    "            batch_size=int(kwargs[\"batch_size\"]),\n",
    "            warm_start=warm_start,\n",
    "            drop_last=False,\n",
    "        )\n",
    "\n",
    "    def _pred_single_model(self, model, X_test, verbose, **kwargs):\n",
    "        return model.predict(X_test).reshape(-1, 1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can compare the model with TabNet implemented in other two model bases. Note that because of different training routines and randomization, they perform differently."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project will be saved to ../../../../output/sample/2023-08-03-20-56-54-0_sample\n",
      "Dataset size: 153 51 52\n",
      "Data saved to ../../../../output/sample/2023-08-03-20-56-54-0_sample (data.csv and tabular_data.csv).\n",
      "\n",
      "-------------Run PytorchTabular-------------\n",
      "\n",
      "Training TabNet\n",
      "Global seed set to 42\n",
      "2023-08-03 20:56:54,716 - {pytorch_tabular.tabular_model:473} - INFO - Preparing the DataLoaders\n",
      "2023-08-03 20:56:54,717 - {pytorch_tabular.tabular_datamodule:290} - INFO - Setting up the datamodule for regression task\n",
      "2023-08-03 20:56:54,730 - {pytorch_tabular.tabular_model:521} - INFO - Preparing the Model: TabNetModel\n",
      "2023-08-03 20:56:54,749 - {pytorch_tabular.tabular_model:268} - INFO - Preparing the Trainer\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "2023-08-03 20:56:54,794 - {pytorch_tabular.tabular_model:582} - INFO - Training Started\n",
      "\n",
      "  | Name             | Type           | Params\n",
      "----------------------------------------------------\n",
      "0 | _embedding_layer | Identity       | 0     \n",
      "1 | _backbone        | TabNetBackbone | 6.6 K \n",
      "2 | _head            | Identity       | 0     \n",
      "3 | loss             | MSELoss        | 0     \n",
      "----------------------------------------------------\n",
      "6.6 K     Trainable params\n",
      "0         Non-trainable params\n",
      "6.6 K     Total params\n",
      "0.026     Total estimated model params size (MB)\n",
      "Epoch: 1/300, Train loss: 33205.1914, Val loss: 22219.6348, Min val loss: 22219.6348, Epoch time: 0.034s.\n",
      "Epoch: 20/300, Train loss: 33028.0117, Val loss: 22216.9707, Min val loss: 22216.7285, Epoch time: 0.025s.\n",
      "Epoch: 40/300, Train loss: 32941.8203, Val loss: 22216.3438, Min val loss: 22216.3438, Epoch time: 0.025s.\n",
      "Epoch: 60/300, Train loss: 32777.3281, Val loss: 22167.9883, Min val loss: 22167.9883, Epoch time: 0.025s.\n",
      "Epoch: 80/300, Train loss: 32580.4570, Val loss: 22131.2676, Min val loss: 22131.2676, Epoch time: 0.029s.\n",
      "Epoch: 100/300, Train loss: 32528.8887, Val loss: 22071.9551, Min val loss: 22071.9551, Epoch time: 0.020s.\n",
      "Epoch: 120/300, Train loss: 32205.6074, Val loss: 22042.0449, Min val loss: 22042.0449, Epoch time: 0.025s.\n",
      "Epoch: 140/300, Train loss: 31967.4121, Val loss: 21952.7188, Min val loss: 21952.7188, Epoch time: 0.020s.\n",
      "Epoch: 160/300, Train loss: 31633.8496, Val loss: 21932.9355, Min val loss: 21901.7578, Epoch time: 0.022s.\n",
      "Epoch: 180/300, Train loss: 31378.5781, Val loss: 21808.1641, Min val loss: 21808.1641, Epoch time: 0.030s.\n",
      "Epoch: 200/300, Train loss: 31127.3535, Val loss: 21683.8438, Min val loss: 21683.8438, Epoch time: 0.025s.\n",
      "Epoch: 220/300, Train loss: 30613.4648, Val loss: 21568.2227, Min val loss: 21568.2227, Epoch time: 0.020s.\n",
      "Epoch: 240/300, Train loss: 30290.5156, Val loss: 21375.5645, Min val loss: 21375.5645, Epoch time: 0.031s.\n",
      "Epoch: 260/300, Train loss: 30013.7715, Val loss: 21076.7520, Min val loss: 21076.7520, Epoch time: 0.032s.\n",
      "Epoch: 280/300, Train loss: 29553.8594, Val loss: 20847.9336, Min val loss: 20847.9336, Epoch time: 0.020s.\n",
      "Epoch: 300/300, Train loss: 29121.6504, Val loss: 20492.7715, Min val loss: 20492.7715, Epoch time: 0.019s.\n",
      "`Trainer.fit` stopped: `max_epochs=300` reached.\n",
      "2023-08-03 20:57:05,597 - {pytorch_tabular.tabular_model:584} - INFO - Training the model completed\n",
      "2023-08-03 20:57:05,597 - {pytorch_tabular.tabular_model:1258} - INFO - Loading the best model\n",
      "/home/xlluo/anaconda3/envs/mlfatigue/lib/python3.8/site-packages/pytorch_lightning/utilities/cloud_io.py:33: LightningDeprecationWarning: `pytorch_lightning.utilities.cloud_io.get_filesystem` has been deprecated in v1.8.0 and will be removed in v1.10.0. Please use `lightning_lite.utilities.cloud_io.get_filesystem` instead.\n",
      "  rank_zero_deprecation(\n",
      "Training mse loss: 28755.66856\n",
      "Validation mse loss: 20492.77134\n",
      "Testing mse loss: 28709.13190\n",
      "Trainer saved. To load the trainer, run trainer = load_trainer(path='../../../../output/sample/2023-08-03-20-56-54-0_sample/trainer.pkl')\n",
      "\n",
      "-------------PytorchTabular End-------------\n",
      "\n",
      "\n",
      "-------------Run WideDeep-------------\n",
      "\n",
      "Training TabNet\n",
      "Epoch: 1/300, Train loss: 33116.3516, Val loss: 22297.6836, Min val loss: 22297.6836\n",
      "Epoch: 21/300, Train loss: 33103.9102, Val loss: 22202.8848, Min val loss: 22202.8848\n",
      "Epoch: 41/300, Train loss: 32987.5508, Val loss: 22197.6016, Min val loss: 22173.0039\n",
      "Epoch: 61/300, Train loss: 32857.5195, Val loss: 22219.0371, Min val loss: 22173.0039\n",
      "Epoch: 81/300, Train loss: 32696.7031, Val loss: 22138.1562, Min val loss: 22138.1562\n",
      "Epoch: 101/300, Train loss: 32594.1836, Val loss: 22073.3223, Min val loss: 22073.3223\n",
      "Epoch: 121/300, Train loss: 32226.0039, Val loss: 21992.0586, Min val loss: 21992.0586\n",
      "Epoch: 141/300, Train loss: 31876.2422, Val loss: 21951.5879, Min val loss: 21951.5879\n",
      "Epoch: 161/300, Train loss: 31858.6699, Val loss: 21798.7676, Min val loss: 21798.7676\n",
      "Epoch: 181/300, Train loss: 31530.3730, Val loss: 21866.0645, Min val loss: 21764.2246\n",
      "Epoch: 201/300, Train loss: 31178.3164, Val loss: 21770.6523, Min val loss: 21764.2246\n",
      "Epoch: 221/300, Train loss: 30669.3008, Val loss: 21807.6797, Min val loss: 21690.2441\n",
      "Epoch: 241/300, Train loss: 30142.0586, Val loss: 21653.9805, Min val loss: 21653.9805\n",
      "Epoch: 261/300, Train loss: 29866.1836, Val loss: 21253.3848, Min val loss: 21253.3848\n",
      "Epoch: 281/300, Train loss: 29291.9355, Val loss: 21164.7910, Min val loss: 21164.7910\n",
      "Restoring model weights from the end of the best epoch\n",
      "Training mse loss: 28909.84837\n",
      "Validation mse loss: 20980.58058\n",
      "Testing mse loss: 27879.31792\n",
      "Trainer saved. To load the trainer, run trainer = load_trainer(path='../../../../output/sample/2023-08-03-20-56-54-0_sample/trainer.pkl')\n",
      "\n",
      "-------------WideDeep End-------------\n",
      "\n",
      "\n",
      "-------------Run TabNet-------------\n",
      "\n",
      "Training TabNet\n",
      "/home/xlluo/anaconda3/envs/mlfatigue/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "epoch 0  | loss: 33144.85156| val_0_mse: 22341.240234375|  0:00:00s\n",
      "epoch 20 | loss: 32885.51953| val_0_mse: 22214.478515625|  0:00:01s\n",
      "epoch 40 | loss: 32737.57617| val_0_mse: 22137.6796875|  0:00:01s\n",
      "epoch 60 | loss: 32582.61719| val_0_mse: 22096.748046875|  0:00:02s\n",
      "epoch 80 | loss: 32351.18945| val_0_mse: 21979.875|  0:00:03s\n",
      "epoch 100| loss: 32111.2168| val_0_mse: 21763.06640625|  0:00:03s\n",
      "epoch 120| loss: 31825.37305| val_0_mse: 21740.693359375|  0:00:04s\n",
      "epoch 140| loss: 31688.35352| val_0_mse: 21717.806640625|  0:00:04s\n",
      "epoch 160| loss: 31341.25586| val_0_mse: 21683.65234375|  0:00:05s\n",
      "epoch 180| loss: 31192.10547| val_0_mse: 21641.650390625|  0:00:05s\n",
      "epoch 200| loss: 30943.33398| val_0_mse: 21454.279296875|  0:00:06s\n",
      "epoch 220| loss: 30890.73242| val_0_mse: 21354.837890625|  0:00:06s\n",
      "epoch 240| loss: 30621.0 | val_0_mse: 21330.0078125|  0:00:07s\n",
      "epoch 260| loss: 30217.78711| val_0_mse: 21175.248046875|  0:00:07s\n",
      "epoch 280| loss: 30123.93164| val_0_mse: 21097.94140625|  0:00:08s\n",
      "Stop training because you reached max_epochs = 300 with best_epoch = 299 and best_val_0_mse = 21033.9609375\n",
      "/home/xlluo/anaconda3/envs/mlfatigue/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "Training mse loss: 29592.75781\n",
      "Validation mse loss: 21033.96094\n",
      "Testing mse loss: 27251.85547\n",
      "Trainer saved. To load the trainer, run trainer = load_trainer(path='../../../../output/sample/2023-08-03-20-56-54-0_sample/trainer.pkl')\n",
      "\n",
      "-------------TabNet End-------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tabensemb.trainer import Trainer\n",
    "from tabensemb.model import PytorchTabular, WideDeep\n",
    "\n",
    "trainer = Trainer(device=\"cpu\")\n",
    "trainer.load_config(\"sample\")\n",
    "trainer.load_data()\n",
    "trainer.add_modelbases(\n",
    "    [PytorchTabular(trainer, model_subset=[\"TabNet\"]), WideDeep(trainer, model_subset=[\"TabNet\"]), TabNet(trainer)])\n",
    "trainer.train(stderr_to_stdout=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PytorchTabular metrics\n",
      "TabNet 1/1\n",
      "WideDeep metrics\n",
      "TabNet 1/1\n",
      "TabNet metrics\n",
      "TabNet 1/1\n",
      "Trainer saved. To load the trainer, run trainer = load_trainer(path='../../../../output/sample/2023-08-03-20-56-54-0_sample/trainer.pkl')\n"
     ]
    },
    {
     "data": {
      "text/plain": "          Program   Model  Training RMSE  Training MSE  Training MAE  \\\n0          TabNet  TabNet     172.025460  29592.758915    137.818516   \n1        WideDeep  TabNet     170.028963  28909.848366    136.487055   \n2  PytorchTabular  TabNet     169.574964  28755.668560    135.228814   \n\n   Training MAPE  Training R2  Training MEDIAN_ABSOLUTE_ERROR  \\\n0       1.024958     0.101657                      116.960863   \n1       0.993000     0.122388                      113.842250   \n2       0.977328     0.127068                      109.420505   \n\n   Training EXPLAINED_VARIANCE_SCORE  Testing RMSE  ...  Testing R2  \\\n0                           0.107024    165.081366  ...    0.080482   \n1                           0.126011    166.971009  ...    0.059311   \n2                           0.127485    169.437693  ...    0.031311   \n\n   Testing MEDIAN_ABSOLUTE_ERROR  Testing EXPLAINED_VARIANCE_SCORE  \\\n0                     115.368511                          0.085757   \n1                     112.697731                          0.065377   \n2                     114.269955                          0.044738   \n\n   Validation RMSE  Validation MSE  Validation MAE  Validation MAPE  \\\n0       145.030899    21033.961806      115.993113         0.950409   \n1       144.846749    20980.580583      117.191995         1.011844   \n2       143.152965    20492.771338      118.205225         1.032690   \n\n   Validation R2  Validation MEDIAN_ABSOLUTE_ERROR  \\\n0       0.050059                         84.365160   \n1       0.052470                         91.948082   \n2       0.074501                         95.991233   \n\n   Validation EXPLAINED_VARIANCE_SCORE  \n0                             0.053165  \n1                             0.054091  \n2                             0.074509  \n\n[3 rows x 23 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Program</th>\n      <th>Model</th>\n      <th>Training RMSE</th>\n      <th>Training MSE</th>\n      <th>Training MAE</th>\n      <th>Training MAPE</th>\n      <th>Training R2</th>\n      <th>Training MEDIAN_ABSOLUTE_ERROR</th>\n      <th>Training EXPLAINED_VARIANCE_SCORE</th>\n      <th>Testing RMSE</th>\n      <th>...</th>\n      <th>Testing R2</th>\n      <th>Testing MEDIAN_ABSOLUTE_ERROR</th>\n      <th>Testing EXPLAINED_VARIANCE_SCORE</th>\n      <th>Validation RMSE</th>\n      <th>Validation MSE</th>\n      <th>Validation MAE</th>\n      <th>Validation MAPE</th>\n      <th>Validation R2</th>\n      <th>Validation MEDIAN_ABSOLUTE_ERROR</th>\n      <th>Validation EXPLAINED_VARIANCE_SCORE</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>TabNet</td>\n      <td>TabNet</td>\n      <td>172.025460</td>\n      <td>29592.758915</td>\n      <td>137.818516</td>\n      <td>1.024958</td>\n      <td>0.101657</td>\n      <td>116.960863</td>\n      <td>0.107024</td>\n      <td>165.081366</td>\n      <td>...</td>\n      <td>0.080482</td>\n      <td>115.368511</td>\n      <td>0.085757</td>\n      <td>145.030899</td>\n      <td>21033.961806</td>\n      <td>115.993113</td>\n      <td>0.950409</td>\n      <td>0.050059</td>\n      <td>84.365160</td>\n      <td>0.053165</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>WideDeep</td>\n      <td>TabNet</td>\n      <td>170.028963</td>\n      <td>28909.848366</td>\n      <td>136.487055</td>\n      <td>0.993000</td>\n      <td>0.122388</td>\n      <td>113.842250</td>\n      <td>0.126011</td>\n      <td>166.971009</td>\n      <td>...</td>\n      <td>0.059311</td>\n      <td>112.697731</td>\n      <td>0.065377</td>\n      <td>144.846749</td>\n      <td>20980.580583</td>\n      <td>117.191995</td>\n      <td>1.011844</td>\n      <td>0.052470</td>\n      <td>91.948082</td>\n      <td>0.054091</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>PytorchTabular</td>\n      <td>TabNet</td>\n      <td>169.574964</td>\n      <td>28755.668560</td>\n      <td>135.228814</td>\n      <td>0.977328</td>\n      <td>0.127068</td>\n      <td>109.420505</td>\n      <td>0.127485</td>\n      <td>169.437693</td>\n      <td>...</td>\n      <td>0.031311</td>\n      <td>114.269955</td>\n      <td>0.044738</td>\n      <td>143.152965</td>\n      <td>20492.771338</td>\n      <td>118.205225</td>\n      <td>1.032690</td>\n      <td>0.074501</td>\n      <td>95.991233</td>\n      <td>0.074509</td>\n    </tr>\n  </tbody>\n</table>\n<p>3 rows × 23 columns</p>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.get_leaderboard()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## More customizations\n",
    "\n",
    "As the base class of all model bases, `AbstractModel` divides key functions into segmentations so that developers can modify almost all of them for customized usages. Some \"high-level\" ones are introduced here. For \"low-level\" (fundamental) ones, interested readers may refer to the source code and API docs of `AbstractModel`."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}