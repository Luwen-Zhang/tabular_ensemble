{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Customized model base\n",
    "\n",
    "For researchers or model base developers, the basic need is comparing their own models with existing benchmarks in `tabensemb`. In this part, a model base is built within the framework assuming that we want to integrate `TabNet` ([from dreamquark-ai team](https://github.com/dreamquark-ai/tabnet)) into `tabensemb` (indeed `pytorch_tabular` and `pytorch_widedeep` have done that) for regression and classification tasks.\n",
    "\n",
    "**Remark**: For `PyTorch`-based models, we have implemented most requirements of the framework so that users can integrate `torch.nn.Module`s more conveniently."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Example: Implement TabNet as a model base from scratch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import tabensemb\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "from tempfile import TemporaryDirectory\n",
    "\n",
    "temp_path = TemporaryDirectory()\n",
    "tabensemb.setting[\"default_output_path\"] = os.path.join(temp_path.name, \"output\")\n",
    "tabensemb.setting[\"default_config_path\"] = os.path.join(temp_path.name, \"configs\")\n",
    "tabensemb.setting[\"default_data_path\"] = os.path.join(temp_path.name, \"data\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "All model bases inherit `AbstractModel` and implement methods within the class. If necessary methods are not implemented, `NotImplementedError` will be raised during usage."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from tabensemb.model import AbstractModel"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We use `scikit-optimize` (https://github.com/scikit-optimize/scikit-optimize) to do Bayesian hyperparameter optimization, so space classes are imported."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from skopt.space import Integer, Real, Categorical"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "First, we define the initialization of the model base. Always remember to pass all args and kwargs to `__init__` of `AbstractModel`. You can do other things in `__init__`. All `*args` and `**kwargs` (including arguments like the `some_param` shown below) are recorded in `self.init_params`.\n",
    "\n",
    "```python\n",
    "class TabNetFromAbstract(AbstractModel):\n",
    "    def __init__(self, *args, some_param=1.1, **kwargs):\n",
    "        super(TabNetFromAbstract, self).__init__(*args, **kwargs)\n",
    "        # Do something else here\n",
    "        self.some_param = some_param\n",
    "        print(self.init_params)\n",
    "```\n",
    "\n",
    "We should define the name of the model base and all available models in the model base.\n",
    "\n",
    "```python\n",
    "    def _get_program_name(self):\n",
    "        return \"TabNetFromAbstract\"\n",
    "\n",
    "    def _get_model_names(self):\n",
    "        return [\"TabNet\"]\n",
    "```\n",
    "\n",
    "For each model in the model base, the program will request initial hyperparameters of the model and their search spaces. They are defined as\n",
    "\n",
    "```python\n",
    "    def _space(self, model_name):\n",
    "        return [\n",
    "            Integer(low=4, high=16, prior=\"uniform\", name=\"n_d\", dtype=int),\n",
    "            Integer(low=4, high=16, prior=\"uniform\", name=\"n_a\", dtype=int),\n",
    "            Integer(low=1, high=6, prior=\"uniform\", name=\"n_steps\", dtype=int),\n",
    "            Real(low=1.0, high=1.5, prior=\"uniform\", name=\"gamma\"),\n",
    "            Integer(\n",
    "                low=1, high=4, prior=\"uniform\", name=\"n_independent\", dtype=int\n",
    "            ),\n",
    "            Integer(low=1, high=4, prior=\"uniform\", name=\"n_shared\", dtype=int),\n",
    "        ] + self.trainer.SPACE\n",
    "\n",
    "    def _initial_values(self, model_name):\n",
    "        return {\n",
    "            \"n_d\": 8,\n",
    "            \"n_a\": 8,\n",
    "            \"n_steps\": 3,\n",
    "            \"gamma\": 1.3,\n",
    "            \"n_independent\": 2,\n",
    "            \"n_shared\": 2,\n",
    "            \"lr\": self.trainer.args[\"lr\"],\n",
    "            \"weight_decay\": self.trainer.args[\"weight_decay\"],\n",
    "            \"batch_size\": self.trainer.args[\"batch_size\"],\n",
    "        }\n",
    "```\n",
    "\n",
    "Before training, each model base has its own way of processing the dataset.\n",
    "\n",
    "`_train_data_preprocess` will return the processed dataset according to a given `Trainer` which provides all training information and data required. In this example, `X_train/X_val/X_test` represent training/validation/testing sets, and `y_train/y_val/y_test` represent corresponding labels.\n",
    "\n",
    "**Remark**: The tabular dataset has gone through all processing stages defined in the `DataModule` inside the trainer **except scaling**. Call `self.trainer.datamodule.data_transform(df, scaler_only=True)` to scale it using the trained scaler if no scaling stage is defined internally in the model.\n",
    "\n",
    "```python\n",
    "    def _train_data_preprocess(self, model_name):\n",
    "        data = self.trainer.datamodule\n",
    "        all_feature_names = data.all_feature_names\n",
    "\n",
    "        X_train = data.data_transform(data.X_train, scaler_only=True)[\n",
    "            all_feature_names\n",
    "        ].values.astype(np.float32)\n",
    "        X_val = data.data_transform(data.X_val, scaler_only=True)[\n",
    "            all_feature_names\n",
    "        ].values.astype(np.float32)\n",
    "        X_test = data.data_transform(data.X_test, scaler_only=True)[\n",
    "            all_feature_names\n",
    "        ].values.astype(np.float32)\n",
    "        y_train = data.y_train.astype(np.float32)\n",
    "        y_val = data.y_val.astype(np.float32)\n",
    "        y_test = data.y_test.astype(np.float32)\n",
    "\n",
    "        return {\n",
    "            \"X_train\": X_train,\n",
    "            \"y_train\": y_train,\n",
    "            \"X_val\": X_val,\n",
    "            \"y_val\": y_val,\n",
    "            \"X_test\": X_test,\n",
    "            \"y_test\": y_test,\n",
    "        }\n",
    "```\n",
    "\n",
    "Correspondingly, `_data_preprocess` will process an upcoming new dataset, including the tabular data `df` containing continuous features and categorical features, and unstacked derived data `derived_data` (multi-modal data or something else depending on the configuration introduced in \"Using data functionalities\"). The returned value should have the same structure as the `X_test` returned in `_train_data_preprocess`.\n",
    "\n",
    "```python\n",
    "    def _data_preprocess(self, df, derived_data, model_name):\n",
    "        return self.trainer.datamodule.data_transform(df, scaler_only=True)[\n",
    "            self.trainer.all_feature_names\n",
    "        ].values.astype(np.float32)\n",
    "```\n",
    "\n",
    "The program will pass a selected set of hyperparameters as `kwargs` to initialize a model, train a model, and predict using the model. The returned `model` will be stored locally and reloaded for evaluation and inference, so make sure it contains all the information needed to make predictions.\n",
    "\n",
    "Here we initialize the model using information contained in the `DataModule` instance, including the indices of categorical features `cat_idxs`, the number of categories of each categorical feature `cat_dims`, the current task `task` (possible values are \"regression\", \"binary\", and \"multiclass\"), the device to train the model `self.trainer.device`, and the hyperparameters `kwargs`. `model_name` is ignored because we only have one model in the model base. All model bases should at least follow the guidance of `self.trainer.device`, `self.trainer.datamodule.task`, `model_name`, and `kwargs` to make all models trained in a consistent way within the framework.\n",
    "\n",
    "```python\n",
    "    def _new_model(self, model_name, verbose, **kwargs):\n",
    "        from pytorch_tabnet.tab_model import TabNetRegressor, TabNetClassifier\n",
    "\n",
    "        datamodule = self.trainer.datamodule\n",
    "        cat_idxs = [\n",
    "            datamodule.all_feature_names.index(x)\n",
    "            for x in datamodule.get_feature_names_by_type(\"Categorical\")\n",
    "        ]\n",
    "        cat_dims = [\n",
    "            datamodule.cat_num_unique[x]\n",
    "            for x in datamodule.get_feature_idx_by_type(\"Categorical\")\n",
    "        ]\n",
    "        self.task = datamodule.task\n",
    "        init_kwargs = dict(\n",
    "            verbose=tabensemb.setting[\"verbose_per_epoch\"] if verbose else 0,\n",
    "            optimizer_params={\n",
    "                \"lr\": kwargs[\"lr\"],\n",
    "                \"weight_decay\": kwargs[\"weight_decay\"],\n",
    "            },\n",
    "            cat_idxs=cat_idxs,\n",
    "            cat_dims=cat_dims,\n",
    "            cat_emb_dim=3,\n",
    "            device_name=self.trainer.device,\n",
    "        )\n",
    "        if self.trainer.datamodule.task == \"regression\":\n",
    "            model = TabNetRegressor(**init_kwargs)\n",
    "        else:\n",
    "            model = TabNetClassifier(**init_kwargs)\n",
    "\n",
    "        model.set_params(\n",
    "            **{\n",
    "                \"n_d\": kwargs[\"n_d\"],\n",
    "                \"n_a\": kwargs[\"n_a\"],\n",
    "                \"n_steps\": kwargs[\"n_steps\"],\n",
    "                \"gamma\": kwargs[\"gamma\"],\n",
    "                \"n_independent\": kwargs[\"n_independent\"],\n",
    "                \"n_shared\": kwargs[\"n_shared\"],\n",
    "            }\n",
    "        )\n",
    "        return model\n",
    "```\n",
    "\n",
    "**Remark**: `kwargs` has all keys defined in `_initial_values`. If a parameter named `batch_size` is included, a new key named `original_batch_size` exists in `kwargs`. The values of `batch_size` and `original_batch_size` may be different if the program finds that the batch size will make the mini-batches tiny. The threshold is defined by `self.limit_batch_size` (default to 6). A tiny batch might interrupt some models, so it is better to use the modified `batch_size` value.\n",
    "\n",
    "The framework will pass `X_train`, `y_train`, `X_val`, and `y_val` from `_train_data_preprocess` to the following `_train_single_model` method, along with some other arguments stating the current training stage. `epoch` is the number of epochs to train the model. `warm_start=True` means the passed model is already trained and should be fine-tuned based on a new dataset. `in_bayes_opt=True` means that the passed `kwargs` is selected by a bayesian hyperparameter optimization step, and a simplified training routine is needed to reduce optimization time, so we set the `max_epochs` to \"bayes_epoch\" in the configuration.\n",
    "\n",
    "**Remark**: `epoch` will be `self.trainer.args[\"bayes_epoch\"]` if `in_bayes_opt=True`, and `self.trainer.args[\"epoch\"]` otherwise.\n",
    "\n",
    "```python\n",
    "    def _train_single_model(\n",
    "        self,\n",
    "        model,\n",
    "        epoch,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        X_val,\n",
    "        y_val,\n",
    "        verbose,\n",
    "        warm_start,\n",
    "        in_bayes_opt,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        eval_set = [(X_val, y_val if self.task == \"regression\" else y_val.flatten())]\n",
    "\n",
    "        model.fit(\n",
    "            X_train,\n",
    "            y_train if self.task == \"regression\" else y_train.flatten(),\n",
    "            eval_set=eval_set,\n",
    "            max_epochs=epoch if not in_bayes_opt else self.trainer.args[\"bayes_epoch\"],\n",
    "            patience=self.trainer.args[\"patience\"],\n",
    "            loss_fn=torch.nn.MSELoss()\n",
    "            if self.task == \"regression\"\n",
    "            else torch.nn.CrossEntropyLoss(),\n",
    "            eval_metric=[\"mse\" if self.task == \"regression\" else \"logloss\"],\n",
    "            batch_size=int(kwargs[\"batch_size\"]),\n",
    "            warm_start=warm_start,\n",
    "            drop_last=False,\n",
    "        )\n",
    "```\n",
    "\n",
    "To evaluate the model or make use of the model, `_pred_single_model` is defined, and `X_test` processed in `_train_data_preprocess` or `_data_preprocess` is passed as an argument. The returned value should always be a two-dimensional `np.ndarray`. For binary classification tasks, the output is the probability of the positive (1) class, and for multiclass classification, the output is the probability of each class. `AbstractModel` automatically deals with the probabilities for metrics and final outputs.\n",
    "\n",
    "```python\n",
    "    def _pred_single_model(self, model, X_test, verbose, **kwargs):\n",
    "        if self.task == \"regression\":\n",
    "            return model.predict(X_test).reshape(-1, 1)\n",
    "        elif self.task == \"binary\":\n",
    "            return model.predict_proba(X_test)[:, 1].reshape(-1, 1)\n",
    "        else:\n",
    "            return model.predict_proba(X_test)\n",
    "```\n",
    "\n",
    "The full code is as follows:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class TabNetFromAbstract(AbstractModel):\n",
    "    def __init__(self, *args, some_param=1.1, **kwargs):\n",
    "        super(TabNetFromAbstract, self).__init__(*args, **kwargs)\n",
    "        # Do something else here\n",
    "        self.some_param = some_param\n",
    "        print(self.init_params)\n",
    "\n",
    "    def _get_program_name(self):\n",
    "        return \"TabNetFromAbstract\"\n",
    "\n",
    "    def _get_model_names(self):\n",
    "        return [\"TabNet\"]\n",
    "\n",
    "    def _space(self, model_name):\n",
    "        return [\n",
    "            Integer(low=4, high=16, prior=\"uniform\", name=\"n_d\", dtype=int),\n",
    "            Integer(low=4, high=16, prior=\"uniform\", name=\"n_a\", dtype=int),\n",
    "            Integer(low=1, high=6, prior=\"uniform\", name=\"n_steps\", dtype=int),\n",
    "            Real(low=1.0, high=1.5, prior=\"uniform\", name=\"gamma\"),\n",
    "            Integer(\n",
    "                low=1, high=4, prior=\"uniform\", name=\"n_independent\", dtype=int\n",
    "            ),\n",
    "            Integer(low=1, high=4, prior=\"uniform\", name=\"n_shared\", dtype=int),\n",
    "        ] + self.trainer.SPACE\n",
    "\n",
    "    def _initial_values(self, model_name):\n",
    "        return {\n",
    "            \"n_d\": 8,\n",
    "            \"n_a\": 8,\n",
    "            \"n_steps\": 3,\n",
    "            \"gamma\": 1.3,\n",
    "            \"n_independent\": 2,\n",
    "            \"n_shared\": 2,\n",
    "            \"lr\": self.trainer.args[\"lr\"],\n",
    "            \"weight_decay\": self.trainer.args[\"weight_decay\"],\n",
    "            \"batch_size\": self.trainer.args[\"batch_size\"],\n",
    "        }\n",
    "\n",
    "    def _train_data_preprocess(self, model_name):\n",
    "        data = self.trainer.datamodule\n",
    "        all_feature_names = data.all_feature_names\n",
    "\n",
    "        X_train = data.data_transform(data.X_train, scaler_only=True)[\n",
    "            all_feature_names\n",
    "        ].values.astype(np.float32)\n",
    "        X_val = data.data_transform(data.X_val, scaler_only=True)[\n",
    "            all_feature_names\n",
    "        ].values.astype(np.float32)\n",
    "        X_test = data.data_transform(data.X_test, scaler_only=True)[\n",
    "            all_feature_names\n",
    "        ].values.astype(np.float32)\n",
    "        y_train = data.y_train.astype(np.float32)\n",
    "        y_val = data.y_val.astype(np.float32)\n",
    "        y_test = data.y_test.astype(np.float32)\n",
    "\n",
    "        return {\n",
    "            \"X_train\": X_train,\n",
    "            \"y_train\": y_train,\n",
    "            \"X_val\": X_val,\n",
    "            \"y_val\": y_val,\n",
    "            \"X_test\": X_test,\n",
    "            \"y_test\": y_test,\n",
    "        }\n",
    "\n",
    "    def _data_preprocess(self, df, derived_data, model_name):\n",
    "        return self.trainer.datamodule.data_transform(df, scaler_only=True)[\n",
    "            self.trainer.all_feature_names\n",
    "        ].values.astype(np.float32)\n",
    "\n",
    "    def _new_model(self, model_name, verbose, **kwargs):\n",
    "        from pytorch_tabnet.tab_model import TabNetRegressor, TabNetClassifier\n",
    "\n",
    "        datamodule = self.trainer.datamodule\n",
    "        cat_idxs = [\n",
    "            datamodule.all_feature_names.index(x)\n",
    "            for x in datamodule.get_feature_names_by_type(\"Categorical\")\n",
    "        ]\n",
    "        cat_dims = [\n",
    "            datamodule.cat_num_unique[x]\n",
    "            for x in datamodule.get_feature_idx_by_type(\"Categorical\")\n",
    "        ]\n",
    "        self.task = datamodule.task\n",
    "        init_kwargs = dict(\n",
    "            verbose=tabensemb.setting[\"verbose_per_epoch\"] if verbose else 0,\n",
    "            optimizer_params={\n",
    "                \"lr\": kwargs[\"lr\"],\n",
    "                \"weight_decay\": kwargs[\"weight_decay\"],\n",
    "            },\n",
    "            cat_idxs=cat_idxs,\n",
    "            cat_dims=cat_dims,\n",
    "            cat_emb_dim=3,\n",
    "            device_name=self.trainer.device,\n",
    "        )\n",
    "        if self.trainer.datamodule.task == \"regression\":\n",
    "            model = TabNetRegressor(**init_kwargs)\n",
    "        else:\n",
    "            model = TabNetClassifier(**init_kwargs)\n",
    "\n",
    "        model.set_params(\n",
    "            **{\n",
    "                \"n_d\": kwargs[\"n_d\"],\n",
    "                \"n_a\": kwargs[\"n_a\"],\n",
    "                \"n_steps\": kwargs[\"n_steps\"],\n",
    "                \"gamma\": kwargs[\"gamma\"],\n",
    "                \"n_independent\": kwargs[\"n_independent\"],\n",
    "                \"n_shared\": kwargs[\"n_shared\"],\n",
    "            }\n",
    "        )\n",
    "        return model\n",
    "\n",
    "    def _train_single_model(\n",
    "        self,\n",
    "        model,\n",
    "        epoch,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        X_val,\n",
    "        y_val,\n",
    "        verbose,\n",
    "        warm_start,\n",
    "        in_bayes_opt,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        eval_set = [(X_val, y_val if self.task == \"regression\" else y_val.flatten())]\n",
    "\n",
    "        model.fit(\n",
    "            X_train,\n",
    "            y_train if self.task == \"regression\" else y_train.flatten(),\n",
    "            eval_set=eval_set,\n",
    "            max_epochs=epoch if not in_bayes_opt else self.trainer.args[\"bayes_epoch\"],\n",
    "            patience=self.trainer.args[\"patience\"],\n",
    "            loss_fn=torch.nn.MSELoss()\n",
    "            if self.task == \"regression\"\n",
    "            else torch.nn.CrossEntropyLoss(),\n",
    "            eval_metric=[\"mse\" if self.task == \"regression\" else \"logloss\"],\n",
    "            batch_size=int(kwargs[\"batch_size\"]),\n",
    "            warm_start=warm_start,\n",
    "            drop_last=False,\n",
    "        )\n",
    "\n",
    "    def _pred_single_model(self, model, X_test, verbose, **kwargs):\n",
    "        if self.task == \"regression\":\n",
    "            return model.predict(X_test).reshape(-1, 1)\n",
    "        elif self.task == \"binary\":\n",
    "            return model.predict_proba(X_test)[:, 1].reshape(-1, 1)\n",
    "        else:\n",
    "            return model.predict_proba(X_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Example: Implement TabNet as a `PyTorch`-based model\n",
    "\n",
    "Indeed, the example shown above uses `TabNetRegressor` and `TabNetClassifier` from `pytorch_tabnet` that have already implemented the training and evaluation procedures over the `torch.nn.Module` subclass called `TabNet`. We can also directly build a model base for `nn.Module`s with less effort. These model bases inherit `TorchModel`, and `nn.Module`s should inherit `AbstractNN` (just needs to change a few lines to migrate previous code into this framework)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from tabensemb.model import TorchModel, AbstractNN\n",
    "from pytorch_tabnet.tab_network import TabNet\n",
    "from typing import Dict"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "First, we implement an `AbstractNN` (which inherits `pytorch_lightning.LightningModule` that further inherits `torch.nn.Module`).\n",
    "\n",
    "We initialize the model in `__init__`. `kwargs` will depend on the arguments passed from `_new_model`, which will be implemented later, but at least it should contain all keys defined in `_initial_values`, as introduced in an above remark.\n",
    "\n",
    "Remember to call `super().__init__`. There is nothing more difficult than initializing a `LightningModule`.\n",
    "\n",
    "We can use `self.hparams.some_param` to get a hyperparameter (equivalent to `kwargs[\"some_param\"]`) if you call `super().__init__(datamodule, **kwargs)` instead of `super().__init__(datamodule)` because `AbstractNN` uses the `LightningModule.save_hyperparameters` utility (which you should **not** call in your own `__init__`).\n",
    "\n",
    "**Remark**: To migrate existing `nn.Module` code (Part 1)\n",
    "\n",
    "* Change `class SomeModel(nn.Module)` to `class SomeModel(AbstractNN)`.\n",
    "* Change the indices of categorical features to `[0, 1, ..., self.n_cat-1]` and the numbers of unique categories of categorical features to `self.cat_num_unique`.\n",
    "* Change the number of input dimensions to `self.n_cont+self.n_cat` and the number of output dimensions `self.n_outputs`.\n",
    "\n",
    "```python\n",
    "class TabNetNN(AbstractNN):\n",
    "    def __init__(\n",
    "        self,\n",
    "        datamodule,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super(TabNetNN, self).__init__(datamodule, **kwargs)\n",
    "        self.network = TabNet(\n",
    "            input_dim=self.n_cont+self.n_cat,\n",
    "            output_dim=self.n_outputs,\n",
    "            n_d=self.hparams.n_d,\n",
    "            n_a=self.hparams.n_a,\n",
    "            n_steps=self.hparams.n_steps,\n",
    "            gamma=self.hparams.gamma,\n",
    "            cat_idxs=list(range(self.n_cat)),\n",
    "            cat_dims=self.cat_num_unique,\n",
    "            cat_emb_dim=[3] * self.n_cat,\n",
    "            n_independent=self.hparams.n_independent,\n",
    "            n_shared=self.hparams.n_shared,\n",
    "        )\n",
    "```\n",
    "\n",
    "Then we implement the computation step of the model. We should implement `_forward` instead of `forward` which is already implemented by `AbstractNN` and is used to automatically process inputs and outputs of `_forward`.\n",
    "\n",
    "There are two input arguments for `_forward`: `x` and `derived_tensors`. `x` is a tensor of continuous features. `derived_tensors` is a dictionary containing contents in `datamodule.derived_data` (which is introduced in the last two sections of the \"Using data functionalities\" part), including categorical data (with the key \"categorical\" if there is any categorical feature), the signal for each data point representing whether it is an augmented one (with the key \"augmented\" if there is any augmented data point), and derived unstacked data (with the key `derived_name` specified in the configuration). This is how multimodal data is passed to a deep learning model in our framework.\n",
    "\n",
    "In the following lines, we build the input of the neural network from the continuous features `x` and the categorical features `derived_tensors[\"categorical\"]` by concatenation (that's why the indices of categorical features are set to `[0, 1, ..., self.n_cat-1]`), calculate the output of the network, and return the output.\n",
    "\n",
    "**Remark**: The default loss function is `torch.nn.MSELoss` for regression, `torch.nn.BCEWithLogitsLoss` for binary classification, and `torch.nn.CrossEntropyLoss` for multiclass classification. To change this behavior, implement `self.loss_fn`. See the \"Advanced customized model base\" part for details.\n",
    "\n",
    "**Remark**: For binary classification tasks, `self.n_outputs=1` so we expect the logits of the positive class (instead of a normalized probability). The output is then used to calculate `torch.nn.BCEWithLogitsLoss` by default. For multiclass classification tasks, `self.n_outputs` is the number of classes, so we expect the logits of these classes (instead of probabilities from `Softmax` or something else). The output is then used to calculate `torch.nn.CrossEntropyLoss` by default.\n",
    "\n",
    "**Remark**: To migrate existing `nn.Module` code (Part 2)\n",
    "\n",
    "* Change `forward` to `_forward`\n",
    "* Get categorical features from `derived_tensors`\n",
    "* Get multimodal features from `derived_tensors` (and load multimodal features using data derivers)\n",
    "* Return logits instead of probabilities\n",
    "\n",
    "```python\n",
    "    def _forward(\n",
    "        self, x: torch.Tensor, derived_tensors: Dict[str, torch.Tensor]\n",
    "    ) -> torch.Tensor:\n",
    "        x_cont = x\n",
    "        if \"categorical\" in derived_tensors.keys():\n",
    "            x_cat = derived_tensors[\"categorical\"]\n",
    "            x_in = torch.concat([x_cat, x_cont], dim=-1)\n",
    "        else:\n",
    "            x_in = x_cont\n",
    "        output, _ = self.network(x_in)\n",
    "        return output\n",
    "```\n",
    "\n",
    "The code is as follows:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class TabNetNN(AbstractNN):\n",
    "    def __init__(\n",
    "        self,\n",
    "        datamodule,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super(TabNetNN, self).__init__(datamodule, **kwargs)\n",
    "        self.network = TabNet(\n",
    "            input_dim=self.n_cont+self.n_cat,\n",
    "            output_dim=self.n_outputs,\n",
    "            n_d=self.hparams.n_d,\n",
    "            n_a=self.hparams.n_a,\n",
    "            n_steps=self.hparams.n_steps,\n",
    "            gamma=self.hparams.gamma,\n",
    "            cat_idxs=list(range(self.n_cat)),\n",
    "            cat_dims=self.cat_num_unique,\n",
    "            cat_emb_dim=[3] * self.n_cat,\n",
    "            n_independent=self.hparams.n_independent,\n",
    "            n_shared=self.hparams.n_shared,\n",
    "        )\n",
    "\n",
    "    def _forward(\n",
    "        self, x: torch.Tensor, derived_tensors: Dict[str, torch.Tensor]\n",
    "    ) -> torch.Tensor:\n",
    "        x_cont = x\n",
    "        if \"categorical\" in derived_tensors.keys():\n",
    "            x_cat = derived_tensors[\"categorical\"]\n",
    "            x_in = torch.concat([x_cat, x_cont], dim=-1)\n",
    "        else:\n",
    "            x_in = x_cont\n",
    "        output, _ = self.network(x_in)\n",
    "        return output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, we build the model base for the neural network. It inherits `TorchModel` which has implemented most required methods. Necessary methods for `TorchModel` can be written similarly with `TabNetFromAbstract`.\n",
    "\n",
    "In the following implementation, `_new_model` passes the datamodule and hyperparameters to the neural network, which is what you saw above in `__init__`. You can also pass other arguments as you want."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "class TabNetFromTorch(TorchModel):\n",
    "    def _new_model(self, model_name, verbose, **kwargs):\n",
    "        return TabNetNN(datamodule=self.trainer.datamodule, **kwargs)\n",
    "\n",
    "    def _get_program_name(self):\n",
    "        return \"TabNetFromTorch\"\n",
    "\n",
    "    def _get_model_names(self):\n",
    "        return [\"TabNet\"]\n",
    "\n",
    "    def _space(self, model_name):\n",
    "        return [\n",
    "            Integer(low=4, high=16, prior=\"uniform\", name=\"n_d\", dtype=int),\n",
    "            Integer(low=4, high=16, prior=\"uniform\", name=\"n_a\", dtype=int),\n",
    "            Integer(low=1, high=6, prior=\"uniform\", name=\"n_steps\", dtype=int),\n",
    "            Real(low=1.0, high=1.5, prior=\"uniform\", name=\"gamma\"),\n",
    "            Integer(\n",
    "                low=1, high=4, prior=\"uniform\", name=\"n_independent\", dtype=int\n",
    "            ),\n",
    "            Integer(low=1, high=4, prior=\"uniform\", name=\"n_shared\", dtype=int),\n",
    "        ] + self.trainer.SPACE\n",
    "\n",
    "    def _initial_values(self, model_name):\n",
    "        return {\n",
    "            \"n_d\": 8,\n",
    "            \"n_a\": 8,\n",
    "            \"n_steps\": 3,\n",
    "            \"gamma\": 1.3,\n",
    "            \"n_independent\": 2,\n",
    "            \"n_shared\": 2,\n",
    "            \"lr\": self.trainer.args[\"lr\"],\n",
    "            \"weight_decay\": self.trainer.args[\"weight_decay\"],\n",
    "            \"batch_size\": self.trainer.args[\"batch_size\"],\n",
    "        }"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Comparison of different implementations in other model bases\n",
    "\n",
    "We can compare our models with TabNet implemented in the other two model bases. Note that because of different training routines and randomization, they perform differently. Let's try the models on a regression task first."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://archive.ics.uci.edu/static/public/9/auto+mpg.zip to /tmp/tmpqjlhi3zp/data/Auto MPG.zip\n",
      "cylinders is Integer and will be treated as a continuous feature.\n",
      "model_year is Integer and will be treated as a continuous feature.\n",
      "origin is Integer and will be treated as a continuous feature.\n",
      "Unknown values are detected in ['horsepower']. They will be treated as np.nan.\n",
      "The project will be saved to /tmp/tmpqjlhi3zp/output/auto-mpg/2023-09-05-20-34-40-0_UserInputConfig\n",
      "Dataset size: 238 80 80\n",
      "Data saved to /tmp/tmpqjlhi3zp/output/auto-mpg/2023-09-05-20-34-40-0_UserInputConfig (data.csv and tabular_data.csv).\n",
      "{'some_param': 1.1, 'program': None, 'model_subset': None, 'exclude_models': None, 'store_in_harddisk': True}\n",
      "\n",
      "-------------Run PytorchTabular-------------\n",
      "\n",
      "Training TabNet\n",
      "Global seed set to 42\n",
      "2023-09-05 20:34:41,058 - {pytorch_tabular.tabular_model:473} - INFO - Preparing the DataLoaders\n",
      "2023-09-05 20:34:41,058 - {pytorch_tabular.tabular_datamodule:290} - INFO - Setting up the datamodule for regression task\n",
      "2023-09-05 20:34:41,066 - {pytorch_tabular.tabular_model:521} - INFO - Preparing the Model: TabNetModel\n",
      "2023-09-05 20:34:41,080 - {pytorch_tabular.tabular_model:268} - INFO - Preparing the Trainer\n",
      "Auto select gpus: [0]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "2023-09-05 20:34:42,005 - {pytorch_tabular.tabular_model:582} - INFO - Training Started\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name             | Type           | Params\n",
      "----------------------------------------------------\n",
      "0 | _embedding_layer | Identity       | 0     \n",
      "1 | _backbone        | TabNetBackbone | 6.1 K \n",
      "2 | _head            | Identity       | 0     \n",
      "3 | loss             | MSELoss        | 0     \n",
      "----------------------------------------------------\n",
      "6.1 K     Trainable params\n",
      "0         Non-trainable params\n",
      "6.1 K     Total params\n",
      "0.024     Total estimated model params size (MB)\n",
      "Epoch: 1/300, Train loss: 631.3760, Val loss: 566.2642, Min val loss: 566.2642, Epoch time: 0.022s.\n",
      "Epoch: 20/300, Train loss: 601.5836, Val loss: 530.7571, Min val loss: 530.7571, Epoch time: 0.020s.\n",
      "Epoch: 40/300, Train loss: 573.0768, Val loss: 512.1393, Min val loss: 512.1393, Epoch time: 0.020s.\n",
      "Epoch: 60/300, Train loss: 547.1718, Val loss: 487.0283, Min val loss: 487.0283, Epoch time: 0.019s.\n",
      "Epoch: 80/300, Train loss: 520.1978, Val loss: 460.7416, Min val loss: 460.7416, Epoch time: 0.021s.\n",
      "Epoch: 100/300, Train loss: 492.8580, Val loss: 435.3112, Min val loss: 435.3112, Epoch time: 0.020s.\n",
      "Epoch: 120/300, Train loss: 458.8178, Val loss: 406.9876, Min val loss: 406.9876, Epoch time: 0.022s.\n",
      "Epoch: 140/300, Train loss: 428.7165, Val loss: 377.8939, Min val loss: 377.8939, Epoch time: 0.020s.\n",
      "Epoch: 160/300, Train loss: 397.3823, Val loss: 349.9593, Min val loss: 349.9593, Epoch time: 0.020s.\n",
      "Epoch: 180/300, Train loss: 370.3346, Val loss: 326.1611, Min val loss: 326.1611, Epoch time: 0.022s.\n",
      "Epoch: 200/300, Train loss: 341.0776, Val loss: 303.4870, Min val loss: 303.4870, Epoch time: 0.020s.\n",
      "Epoch: 220/300, Train loss: 317.4785, Val loss: 279.3075, Min val loss: 279.3075, Epoch time: 0.021s.\n",
      "Epoch: 240/300, Train loss: 291.1337, Val loss: 259.3213, Min val loss: 259.3213, Epoch time: 0.020s.\n",
      "Epoch: 260/300, Train loss: 261.4830, Val loss: 238.6501, Min val loss: 238.6501, Epoch time: 0.021s.\n",
      "Epoch: 280/300, Train loss: 236.9622, Val loss: 214.3747, Min val loss: 214.3747, Epoch time: 0.022s.\n",
      "Epoch: 300/300, Train loss: 212.2588, Val loss: 192.0711, Min val loss: 192.0711, Epoch time: 0.020s.\n",
      "`Trainer.fit` stopped: `max_epochs=300` reached.\n",
      "2023-09-05 20:34:52,257 - {pytorch_tabular.tabular_model:584} - INFO - Training the model completed\n",
      "2023-09-05 20:34:52,258 - {pytorch_tabular.tabular_model:1258} - INFO - Loading the best model\n",
      "/home/xlluo/anaconda3/envs/mlfatigue/lib/python3.8/site-packages/pytorch_lightning/utilities/cloud_io.py:33: LightningDeprecationWarning: `pytorch_lightning.utilities.cloud_io.get_filesystem` has been deprecated in v1.8.0 and will be removed in v1.10.0. Please use `lightning_lite.utilities.cloud_io.get_filesystem` instead.\n",
      "  rank_zero_deprecation(\n",
      "Training mse loss: 204.45271\n",
      "Validation mse loss: 192.07111\n",
      "Testing mse loss: 206.28559\n",
      "Trainer saved. To load the trainer, run trainer = load_trainer(path='/tmp/tmpqjlhi3zp/output/auto-mpg/2023-09-05-20-34-40-0_UserInputConfig/trainer.pkl')\n",
      "\n",
      "-------------PytorchTabular End-------------\n",
      "\n",
      "\n",
      "-------------Run WideDeep-------------\n",
      "\n",
      "Training TabNet\n",
      "Epoch: 1/300, Train loss: 632.3985, Val loss: 567.4961, Min val loss: 567.4961\n",
      "Epoch: 21/300, Train loss: 598.0987, Val loss: 533.1615, Min val loss: 533.1615\n",
      "Epoch: 41/300, Train loss: 569.2328, Val loss: 512.1868, Min val loss: 512.1868\n",
      "Epoch: 61/300, Train loss: 540.6208, Val loss: 484.7702, Min val loss: 484.7702\n",
      "Epoch: 81/300, Train loss: 510.7968, Val loss: 455.2627, Min val loss: 455.2627\n",
      "Epoch: 101/300, Train loss: 479.9683, Val loss: 423.4656, Min val loss: 423.4656\n",
      "Epoch: 121/300, Train loss: 448.9766, Val loss: 398.6732, Min val loss: 398.6732\n",
      "Epoch: 141/300, Train loss: 418.1851, Val loss: 370.4178, Min val loss: 370.4178\n",
      "Epoch: 161/300, Train loss: 388.3375, Val loss: 341.3357, Min val loss: 341.3357\n",
      "Epoch: 181/300, Train loss: 359.7862, Val loss: 309.1912, Min val loss: 309.1912\n",
      "Epoch: 201/300, Train loss: 331.6516, Val loss: 285.8885, Min val loss: 285.8885\n",
      "Epoch: 221/300, Train loss: 304.4613, Val loss: 264.4017, Min val loss: 264.4017\n",
      "Epoch: 241/300, Train loss: 278.0829, Val loss: 238.1585, Min val loss: 238.1585\n",
      "Epoch: 261/300, Train loss: 252.6467, Val loss: 212.2308, Min val loss: 212.2308\n",
      "Epoch: 281/300, Train loss: 227.7788, Val loss: 186.5846, Min val loss: 186.5846\n",
      "Restoring model weights from the end of the best epoch\n",
      "Training mse loss: 197.94063\n",
      "Validation mse loss: 166.17101\n",
      "Testing mse loss: 219.99466\n",
      "Trainer saved. To load the trainer, run trainer = load_trainer(path='/tmp/tmpqjlhi3zp/output/auto-mpg/2023-09-05-20-34-40-0_UserInputConfig/trainer.pkl')\n",
      "\n",
      "-------------WideDeep End-------------\n",
      "\n",
      "\n",
      "-------------Run TabNetFromAbstract-------------\n",
      "\n",
      "Training TabNet\n",
      "/home/xlluo/anaconda3/envs/mlfatigue/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "epoch 0  | loss: 587.08069| val_0_mse: 521.3651123046875|  0:00:00s\n",
      "epoch 20 | loss: 543.03516| val_0_mse: 509.48974609375|  0:00:00s\n",
      "epoch 40 | loss: 503.41489| val_0_mse: 474.2294006347656|  0:00:00s\n",
      "epoch 60 | loss: 464.7016| val_0_mse: 433.50634765625|  0:00:01s\n",
      "epoch 80 | loss: 428.9183| val_0_mse: 394.7861328125|  0:00:01s\n",
      "epoch 100| loss: 397.22137| val_0_mse: 362.9132385253906|  0:00:02s\n",
      "epoch 120| loss: 367.97043| val_0_mse: 331.7356262207031|  0:00:02s\n",
      "epoch 140| loss: 339.42422| val_0_mse: 303.83282470703125|  0:00:03s\n",
      "epoch 160| loss: 310.759 | val_0_mse: 271.6055908203125|  0:00:03s\n",
      "epoch 180| loss: 284.62549| val_0_mse: 246.6285858154297|  0:00:03s\n",
      "epoch 200| loss: 253.08417| val_0_mse: 222.72137451171875|  0:00:04s\n",
      "epoch 220| loss: 226.72321| val_0_mse: 195.1395263671875|  0:00:05s\n",
      "epoch 240| loss: 197.08269| val_0_mse: 170.7641143798828|  0:00:05s\n",
      "epoch 260| loss: 171.98416| val_0_mse: 150.0797119140625|  0:00:06s\n",
      "epoch 280| loss: 146.14322| val_0_mse: 127.56330871582031|  0:00:07s\n",
      "Stop training because you reached max_epochs = 300 with best_epoch = 299 and best_val_0_mse = 107.80545806884766\n",
      "/home/xlluo/anaconda3/envs/mlfatigue/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "Training mse loss: 113.64614\n",
      "Validation mse loss: 107.80546\n",
      "Testing mse loss: 106.64824\n",
      "Trainer saved. To load the trainer, run trainer = load_trainer(path='/tmp/tmpqjlhi3zp/output/auto-mpg/2023-09-05-20-34-40-0_UserInputConfig/trainer.pkl')\n",
      "\n",
      "-------------TabNetFromAbstract End-------------\n",
      "\n",
      "\n",
      "-------------Run TabNetFromTorch-------------\n",
      "\n",
      "Training TabNet\n",
      "Auto select gpus: [0]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                | Type     | Params\n",
      "-------------------------------------------------\n",
      "0 | default_loss_fn     | MSELoss  | 0     \n",
      "1 | default_output_norm | Identity | 0     \n",
      "2 | network             | TabNet   | 6.1 K \n",
      "-------------------------------------------------\n",
      "6.1 K     Trainable params\n",
      "0         Non-trainable params\n",
      "6.1 K     Total params\n",
      "0.024     Total estimated model params size (MB)\n",
      "Epoch: 1/300, Train loss: 631.3761, Val loss: 568.1518, Min val loss: 568.1518, Min ES val loss: 568.1518, Epoch time: 0.035s.\n",
      "Epoch: 20/300, Train loss: 601.5811, Val loss: 531.1486, Min val loss: 531.1486, Min ES val loss: 531.1486, Epoch time: 0.022s.\n",
      "Epoch: 40/300, Train loss: 572.6572, Val loss: 512.5704, Min val loss: 512.5704, Min ES val loss: 512.5704, Epoch time: 0.023s.\n",
      "Epoch: 60/300, Train loss: 547.6562, Val loss: 486.5998, Min val loss: 486.5998, Min ES val loss: 486.5998, Epoch time: 0.022s.\n",
      "Epoch: 80/300, Train loss: 522.5878, Val loss: 458.9257, Min val loss: 458.9257, Min ES val loss: 458.9257, Epoch time: 0.043s.\n",
      "Epoch: 100/300, Train loss: 494.3544, Val loss: 433.1508, Min val loss: 433.1508, Min ES val loss: 433.1508, Epoch time: 0.018s.\n",
      "Epoch: 120/300, Train loss: 463.9587, Val loss: 399.6929, Min val loss: 399.6929, Min ES val loss: 399.6929, Epoch time: 0.019s.\n",
      "Epoch: 140/300, Train loss: 432.1950, Val loss: 374.4449, Min val loss: 374.4449, Min ES val loss: 374.4449, Epoch time: 0.017s.\n",
      "Epoch: 160/300, Train loss: 400.5341, Val loss: 346.9869, Min val loss: 346.9869, Min ES val loss: 346.9869, Epoch time: 0.017s.\n",
      "Epoch: 180/300, Train loss: 372.6822, Val loss: 323.2344, Min val loss: 323.2344, Min ES val loss: 323.2344, Epoch time: 0.018s.\n",
      "Epoch: 200/300, Train loss: 343.3183, Val loss: 298.9082, Min val loss: 298.9082, Min ES val loss: 298.9082, Epoch time: 0.019s.\n",
      "Epoch: 220/300, Train loss: 318.0136, Val loss: 274.2305, Min val loss: 274.2305, Min ES val loss: 274.2305, Epoch time: 0.028s.\n",
      "Epoch: 240/300, Train loss: 294.5093, Val loss: 254.9106, Min val loss: 254.9106, Min ES val loss: 254.9106, Epoch time: 0.018s.\n",
      "Epoch: 260/300, Train loss: 264.5569, Val loss: 232.6060, Min val loss: 232.6060, Min ES val loss: 232.6060, Epoch time: 0.017s.\n",
      "Epoch: 280/300, Train loss: 239.9647, Val loss: 205.4778, Min val loss: 205.4778, Min ES val loss: 205.4778, Epoch time: 0.017s.\n",
      "Epoch: 300/300, Train loss: 214.0363, Val loss: 180.9165, Min val loss: 180.9165, Min ES val loss: 180.9165, Epoch time: 0.017s.\n",
      "`Trainer.fit` stopped: `max_epochs=300` reached.\n",
      "Training mse loss: 189.97484\n",
      "Validation mse loss: 180.91652\n",
      "Testing mse loss: 187.37005\n",
      "Trainer saved. To load the trainer, run trainer = load_trainer(path='/tmp/tmpqjlhi3zp/output/auto-mpg/2023-09-05-20-34-40-0_UserInputConfig/trainer.pkl')\n",
      "\n",
      "-------------TabNetFromTorch End-------------\n",
      "\n",
      "PytorchTabular metrics\n",
      "TabNet 1/1\n",
      "WideDeep metrics\n",
      "TabNet 1/1\n",
      "TabNetFromAbstract metrics\n",
      "TabNet 1/1\n",
      "TabNetFromTorch metrics\n",
      "TabNet 1/1\n",
      "Trainer saved. To load the trainer, run trainer = load_trainer(path='/tmp/tmpqjlhi3zp/output/auto-mpg/2023-09-05-20-34-40-0_UserInputConfig/trainer.pkl')\n"
     ]
    },
    {
     "data": {
      "text/plain": "              Program   Model  Training RMSE  Training MSE  Training MAE  \\\n0  TabNetFromAbstract  TabNet      10.660494    113.646137      9.776540   \n1     TabNetFromTorch  TabNet      13.783136    189.974836     13.013939   \n2      PytorchTabular  TabNet      14.298696    204.452712     13.516386   \n3            WideDeep  TabNet      14.069138    197.940635     13.082123   \n\n   Training MAPE  Training R2  Training MEDIAN_ABSOLUTE_ERROR  \\\n0       0.414020    -0.763090                        9.172017   \n1       0.559457    -1.947242                       11.927944   \n2       0.573844    -2.171850                       12.404514   \n3       0.549435    -2.070823                       12.013724   \n\n   Training EXPLAINED_VARIANCE_SCORE  Testing RMSE  ...  Testing R2  \\\n0                           0.715718     10.327064  ...   -0.983546   \n1                           0.680225     13.688318  ...   -2.484888   \n2                           0.662418     14.362646  ...   -2.836698   \n3                           0.577193     14.832217  ...   -3.091672   \n\n   Testing MEDIAN_ABSOLUTE_ERROR  Testing EXPLAINED_VARIANCE_SCORE  \\\n0                       9.644832                          0.720496   \n1                      12.811994                          0.739114   \n2                      13.419121                          0.587029   \n3                      13.292557                          0.270427   \n\n   Validation RMSE  Validation MSE  Validation MAE  Validation MAPE  \\\n0        10.382940      107.805452        9.493001         0.419796   \n1        13.450521      180.916516       12.600047         0.566158   \n2        13.858972      192.071113       12.810732         0.564365   \n3        12.890733      166.171009       11.790706         0.531309   \n\n   Validation R2  Validation MEDIAN_ABSOLUTE_ERROR  \\\n0      -0.925839                          8.926075   \n1      -2.231896                         11.486443   \n2      -2.431162                         11.688212   \n3      -1.968482                         11.140949   \n\n   Validation EXPLAINED_VARIANCE_SCORE  \n0                             0.631675  \n1                             0.604217  \n2                             0.500589  \n3                             0.245480  \n\n[4 rows x 23 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Program</th>\n      <th>Model</th>\n      <th>Training RMSE</th>\n      <th>Training MSE</th>\n      <th>Training MAE</th>\n      <th>Training MAPE</th>\n      <th>Training R2</th>\n      <th>Training MEDIAN_ABSOLUTE_ERROR</th>\n      <th>Training EXPLAINED_VARIANCE_SCORE</th>\n      <th>Testing RMSE</th>\n      <th>...</th>\n      <th>Testing R2</th>\n      <th>Testing MEDIAN_ABSOLUTE_ERROR</th>\n      <th>Testing EXPLAINED_VARIANCE_SCORE</th>\n      <th>Validation RMSE</th>\n      <th>Validation MSE</th>\n      <th>Validation MAE</th>\n      <th>Validation MAPE</th>\n      <th>Validation R2</th>\n      <th>Validation MEDIAN_ABSOLUTE_ERROR</th>\n      <th>Validation EXPLAINED_VARIANCE_SCORE</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>TabNetFromAbstract</td>\n      <td>TabNet</td>\n      <td>10.660494</td>\n      <td>113.646137</td>\n      <td>9.776540</td>\n      <td>0.414020</td>\n      <td>-0.763090</td>\n      <td>9.172017</td>\n      <td>0.715718</td>\n      <td>10.327064</td>\n      <td>...</td>\n      <td>-0.983546</td>\n      <td>9.644832</td>\n      <td>0.720496</td>\n      <td>10.382940</td>\n      <td>107.805452</td>\n      <td>9.493001</td>\n      <td>0.419796</td>\n      <td>-0.925839</td>\n      <td>8.926075</td>\n      <td>0.631675</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>TabNetFromTorch</td>\n      <td>TabNet</td>\n      <td>13.783136</td>\n      <td>189.974836</td>\n      <td>13.013939</td>\n      <td>0.559457</td>\n      <td>-1.947242</td>\n      <td>11.927944</td>\n      <td>0.680225</td>\n      <td>13.688318</td>\n      <td>...</td>\n      <td>-2.484888</td>\n      <td>12.811994</td>\n      <td>0.739114</td>\n      <td>13.450521</td>\n      <td>180.916516</td>\n      <td>12.600047</td>\n      <td>0.566158</td>\n      <td>-2.231896</td>\n      <td>11.486443</td>\n      <td>0.604217</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>PytorchTabular</td>\n      <td>TabNet</td>\n      <td>14.298696</td>\n      <td>204.452712</td>\n      <td>13.516386</td>\n      <td>0.573844</td>\n      <td>-2.171850</td>\n      <td>12.404514</td>\n      <td>0.662418</td>\n      <td>14.362646</td>\n      <td>...</td>\n      <td>-2.836698</td>\n      <td>13.419121</td>\n      <td>0.587029</td>\n      <td>13.858972</td>\n      <td>192.071113</td>\n      <td>12.810732</td>\n      <td>0.564365</td>\n      <td>-2.431162</td>\n      <td>11.688212</td>\n      <td>0.500589</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>WideDeep</td>\n      <td>TabNet</td>\n      <td>14.069138</td>\n      <td>197.940635</td>\n      <td>13.082123</td>\n      <td>0.549435</td>\n      <td>-2.070823</td>\n      <td>12.013724</td>\n      <td>0.577193</td>\n      <td>14.832217</td>\n      <td>...</td>\n      <td>-3.091672</td>\n      <td>13.292557</td>\n      <td>0.270427</td>\n      <td>12.890733</td>\n      <td>166.171009</td>\n      <td>11.790706</td>\n      <td>0.531309</td>\n      <td>-1.968482</td>\n      <td>11.140949</td>\n      <td>0.245480</td>\n    </tr>\n  </tbody>\n</table>\n<p>4 rows  23 columns</p>\n</div>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tabensemb.trainer import Trainer\n",
    "from tabensemb.config import UserConfig\n",
    "from tabensemb.model import PytorchTabular, WideDeep\n",
    "\n",
    "trainer = Trainer(device=device)\n",
    "mpg_columns = [\n",
    "    \"mpg\",\n",
    "    \"cylinders\",\n",
    "    \"displacement\",\n",
    "    \"horsepower\",\n",
    "    \"weight\",\n",
    "    \"acceleration\",\n",
    "    \"model_year\",\n",
    "    \"origin\",\n",
    "    \"car_name\",\n",
    "]\n",
    "cfg = UserConfig.from_uci(\"Auto MPG\", column_names=mpg_columns, sep=\"\\s+\")\n",
    "trainer.load_config(cfg)\n",
    "trainer.load_data()\n",
    "trainer.add_modelbases(\n",
    "    [\n",
    "        PytorchTabular(trainer, model_subset=[\"TabNet\"]),\n",
    "        WideDeep(trainer, model_subset=[\"TabNet\"]),\n",
    "        TabNetFromAbstract(trainer),\n",
    "        TabNetFromTorch(trainer),\n",
    "    ]\n",
    ")\n",
    "trainer.train(stderr_to_stdout=True)\n",
    "trainer.get_leaderboard()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see that `TabNet` does not perform well with the current hyperparameters. We can use `trainer.args[\"bayes_opt\"] = True` to activate Bayesian hyperparameter optimization to improve its performance.\n",
    "\n",
    "Then the binary classification task:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://archive.ics.uci.edu/static/public/2/adult.zip to /tmp/tmpqjlhi3zp/data/Adult.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xlluo/hdd/tabular_ensemble/tabensemb/config/user_config.py:279: UserWarning: There exists .test file(s) ['adult.test'] which should be used for final metrics. The .zip file is left for the user to process.\n",
      "  warnings.warn(\n",
      "/home/xlluo/hdd/tabular_ensemble/tabensemb/utils/utils.py:461: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  df = pd.read_csv(StringIO(s), names=names, sep=sep)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age is Integer and will be treated as a continuous feature.\n",
      "fnlwgt is Integer and will be treated as a continuous feature.\n",
      "education-num is Integer and will be treated as a continuous feature.\n",
      "capital-gain is Integer and will be treated as a continuous feature.\n",
      "capital-loss is Integer and will be treated as a continuous feature.\n",
      "hours-per-week is Integer and will be treated as a continuous feature.\n",
      "The project will be saved to /tmp/tmpqjlhi3zp/output/adult/2023-09-05-20-35-24-0_UserInputConfig\n",
      "Dataset size: 19536 6512 6513\n",
      "Data saved to /tmp/tmpqjlhi3zp/output/adult/2023-09-05-20-35-24-0_UserInputConfig (data.csv and tabular_data.csv).\n",
      "{'some_param': 1.1, 'program': None, 'model_subset': None, 'exclude_models': None, 'store_in_harddisk': True}\n",
      "\n",
      "-------------Run PytorchTabular-------------\n",
      "\n",
      "Training TabNet\n",
      "Global seed set to 42\n",
      "2023-09-05 20:35:25,711 - {pytorch_tabular.tabular_model:473} - INFO - Preparing the DataLoaders\n",
      "2023-09-05 20:35:25,712 - {pytorch_tabular.tabular_datamodule:290} - INFO - Setting up the datamodule for classification task\n",
      "2023-09-05 20:35:25,762 - {pytorch_tabular.tabular_model:521} - INFO - Preparing the Model: TabNetModel\n",
      "2023-09-05 20:35:25,783 - {pytorch_tabular.tabular_model:268} - INFO - Preparing the Trainer\n",
      "Auto select gpus: [0]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "2023-09-05 20:35:25,796 - {pytorch_tabular.tabular_model:582} - INFO - Training Started\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name             | Type             | Params\n",
      "------------------------------------------------------\n",
      "0 | _embedding_layer | Identity         | 0     \n",
      "1 | _backbone        | TabNetBackbone   | 11.2 K\n",
      "2 | _head            | Identity         | 0     \n",
      "3 | loss             | CrossEntropyLoss | 0     \n",
      "------------------------------------------------------\n",
      "11.2 K    Trainable params\n",
      "0         Non-trainable params\n",
      "11.2 K    Total params\n",
      "0.045     Total estimated model params size (MB)\n",
      "Epoch: 1/300, Train loss: 1.1484, Val loss: 0.7944, Min val loss: 0.7944, Epoch time: 0.660s.\n",
      "Epoch: 20/300, Train loss: 0.4990, Val loss: 0.4745, Min val loss: 0.4745, Epoch time: 0.658s.\n",
      "Epoch: 40/300, Train loss: 0.4284, Val loss: 0.4167, Min val loss: 0.4167, Epoch time: 0.630s.\n",
      "Epoch: 60/300, Train loss: 0.4016, Val loss: 0.3994, Min val loss: 0.3978, Epoch time: 0.661s.\n",
      "Epoch: 80/300, Train loss: 0.3866, Val loss: 0.3913, Min val loss: 0.3896, Epoch time: 0.683s.\n",
      "Epoch: 100/300, Train loss: 0.3802, Val loss: 0.3806, Min val loss: 0.3806, Epoch time: 0.619s.\n",
      "Epoch: 120/300, Train loss: 0.3637, Val loss: 0.3661, Min val loss: 0.3661, Epoch time: 0.797s.\n",
      "Epoch: 140/300, Train loss: 0.3539, Val loss: 0.3600, Min val loss: 0.3600, Epoch time: 0.732s.\n",
      "Epoch: 160/300, Train loss: 0.3467, Val loss: 0.3549, Min val loss: 0.3541, Epoch time: 0.700s.\n",
      "Epoch: 180/300, Train loss: 0.3428, Val loss: 0.3497, Min val loss: 0.3490, Epoch time: 0.746s.\n",
      "Epoch: 200/300, Train loss: 0.3375, Val loss: 0.3451, Min val loss: 0.3451, Epoch time: 0.707s.\n",
      "Epoch: 220/300, Train loss: 0.3343, Val loss: 0.3423, Min val loss: 0.3423, Epoch time: 0.668s.\n",
      "Epoch: 240/300, Train loss: 0.3297, Val loss: 0.3415, Min val loss: 0.3398, Epoch time: 0.669s.\n",
      "Epoch: 260/300, Train loss: 0.3241, Val loss: 0.3431, Min val loss: 0.3391, Epoch time: 0.750s.\n",
      "Epoch: 280/300, Train loss: 0.3212, Val loss: 0.3373, Min val loss: 0.3373, Epoch time: 0.724s.\n",
      "Epoch: 300/300, Train loss: 0.3201, Val loss: 0.3380, Min val loss: 0.3362, Epoch time: 0.646s.\n",
      "`Trainer.fit` stopped: `max_epochs=300` reached.\n",
      "2023-09-05 20:39:01,294 - {pytorch_tabular.tabular_model:584} - INFO - Training the model completed\n",
      "2023-09-05 20:39:01,294 - {pytorch_tabular.tabular_model:1258} - INFO - Loading the best model\n",
      "/home/xlluo/anaconda3/envs/mlfatigue/lib/python3.8/site-packages/pytorch_lightning/utilities/cloud_io.py:33: LightningDeprecationWarning: `pytorch_lightning.utilities.cloud_io.get_filesystem` has been deprecated in v1.8.0 and will be removed in v1.10.0. Please use `lightning_lite.utilities.cloud_io.get_filesystem` instead.\n",
      "  rank_zero_deprecation(\n",
      "Training log_loss loss: 0.31245\n",
      "Validation log_loss loss: 0.33618\n",
      "Testing log_loss loss: 0.33183\n",
      "Trainer saved. To load the trainer, run trainer = load_trainer(path='/tmp/tmpqjlhi3zp/output/adult/2023-09-05-20-35-24-0_UserInputConfig/trainer.pkl')\n",
      "\n",
      "-------------PytorchTabular End-------------\n",
      "\n",
      "\n",
      "-------------Run WideDeep-------------\n",
      "\n",
      "Training TabNet\n",
      "Epoch: 1/300, Train loss: 1.1847, Val loss: 0.9394, Min val loss: 0.9394\n",
      "Epoch: 21/300, Train loss: 0.4870, Val loss: 0.4681, Min val loss: 0.4681\n",
      "Epoch: 41/300, Train loss: 0.4204, Val loss: 0.4097, Min val loss: 0.4097\n",
      "Epoch: 61/300, Train loss: 0.3905, Val loss: 0.3855, Min val loss: 0.3852\n",
      "Epoch: 81/300, Train loss: 0.3784, Val loss: 0.3750, Min val loss: 0.3750\n",
      "Epoch: 101/300, Train loss: 0.3657, Val loss: 0.3704, Min val loss: 0.3704\n",
      "Epoch: 121/300, Train loss: 0.3642, Val loss: 0.3670, Min val loss: 0.3659\n",
      "Epoch: 141/300, Train loss: 0.3569, Val loss: 0.3619, Min val loss: 0.3619\n",
      "Epoch: 161/300, Train loss: 0.3502, Val loss: 0.3583, Min val loss: 0.3576\n",
      "Epoch: 181/300, Train loss: 0.3462, Val loss: 0.3555, Min val loss: 0.3555\n",
      "Epoch: 201/300, Train loss: 0.3392, Val loss: 0.3508, Min val loss: 0.3505\n",
      "Epoch: 221/300, Train loss: 0.3366, Val loss: 0.3490, Min val loss: 0.3481\n",
      "Epoch: 241/300, Train loss: 0.3314, Val loss: 0.3454, Min val loss: 0.3446\n",
      "Epoch: 261/300, Train loss: 0.3282, Val loss: 0.3423, Min val loss: 0.3419\n",
      "Epoch: 281/300, Train loss: 0.3239, Val loss: 0.3412, Min val loss: 0.3412\n",
      "Restoring model weights from the end of the best epoch\n",
      "Training log_loss loss: 0.31902\n",
      "Validation log_loss loss: 0.33853\n",
      "Testing log_loss loss: 0.33015\n",
      "Trainer saved. To load the trainer, run trainer = load_trainer(path='/tmp/tmpqjlhi3zp/output/adult/2023-09-05-20-35-24-0_UserInputConfig/trainer.pkl')\n",
      "\n",
      "-------------WideDeep End-------------\n",
      "\n",
      "\n",
      "-------------Run TabNetFromAbstract-------------\n",
      "\n",
      "Training TabNet\n",
      "/home/xlluo/anaconda3/envs/mlfatigue/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "epoch 0  | loss: 0.78187 | val_0_logloss: 0.64186 |  0:00:00s\n",
      "epoch 20 | loss: 0.41712 | val_0_logloss: 0.41622 |  0:00:10s\n",
      "epoch 40 | loss: 0.37494 | val_0_logloss: 0.38008 |  0:00:19s\n",
      "epoch 60 | loss: 0.35906 | val_0_logloss: 0.36739 |  0:00:28s\n",
      "epoch 80 | loss: 0.34788 | val_0_logloss: 0.35626 |  0:00:38s\n",
      "epoch 100| loss: 0.34006 | val_0_logloss: 0.34914 |  0:00:47s\n",
      "epoch 120| loss: 0.33378 | val_0_logloss: 0.34331 |  0:00:57s\n",
      "epoch 140| loss: 0.32859 | val_0_logloss: 0.34002 |  0:01:06s\n",
      "epoch 160| loss: 0.32314 | val_0_logloss: 0.33474 |  0:01:15s\n",
      "epoch 180| loss: 0.31747 | val_0_logloss: 0.33171 |  0:01:24s\n",
      "epoch 200| loss: 0.31415 | val_0_logloss: 0.33097 |  0:01:34s\n",
      "epoch 220| loss: 0.31176 | val_0_logloss: 0.33033 |  0:01:43s\n",
      "epoch 240| loss: 0.30887 | val_0_logloss: 0.33064 |  0:01:52s\n",
      "epoch 260| loss: 0.30567 | val_0_logloss: 0.33185 |  0:02:01s\n",
      "epoch 280| loss: 0.30409 | val_0_logloss: 0.33116 |  0:02:10s\n",
      "Stop training because you reached max_epochs = 300 with best_epoch = 298 and best_val_0_logloss = 0.32864\n",
      "/home/xlluo/anaconda3/envs/mlfatigue/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "Training log_loss loss: 0.28966\n",
      "Validation log_loss loss: 0.32864\n",
      "Testing log_loss loss: 0.32531\n",
      "Trainer saved. To load the trainer, run trainer = load_trainer(path='/tmp/tmpqjlhi3zp/output/adult/2023-09-05-20-35-24-0_UserInputConfig/trainer.pkl')\n",
      "\n",
      "-------------TabNetFromAbstract End-------------\n",
      "\n",
      "\n",
      "-------------Run TabNetFromTorch-------------\n",
      "\n",
      "Training TabNet\n",
      "Auto select gpus: [0]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                | Type              | Params\n",
      "----------------------------------------------------------\n",
      "0 | default_loss_fn     | BCEWithLogitsLoss | 0     \n",
      "1 | default_output_norm | Sigmoid           | 0     \n",
      "2 | network             | TabNet            | 7.9 K \n",
      "----------------------------------------------------------\n",
      "7.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "7.9 K     Total params\n",
      "0.032     Total estimated model params size (MB)\n",
      "Epoch: 1/300, Train loss: 0.6595, Val loss: 0.6139, Min val loss: 0.6139, Min ES val loss: 0.6139, Epoch time: 0.568s.\n",
      "Epoch: 20/300, Train loss: 0.4368, Val loss: 0.4377, Min val loss: 0.4377, Min ES val loss: 0.4377, Epoch time: 0.535s.\n",
      "Epoch: 40/300, Train loss: 0.3888, Val loss: 0.3839, Min val loss: 0.3839, Min ES val loss: 0.3839, Epoch time: 0.484s.\n",
      "Epoch: 60/300, Train loss: 0.3628, Val loss: 0.3650, Min val loss: 0.3646, Min ES val loss: 0.3646, Epoch time: 0.636s.\n",
      "Epoch: 80/300, Train loss: 0.3512, Val loss: 0.3480, Min val loss: 0.3480, Min ES val loss: 0.3480, Epoch time: 0.509s.\n",
      "Epoch: 100/300, Train loss: 0.3377, Val loss: 0.3421, Min val loss: 0.3421, Min ES val loss: 0.3421, Epoch time: 0.553s.\n",
      "Epoch: 120/300, Train loss: 0.3335, Val loss: 0.3388, Min val loss: 0.3367, Min ES val loss: 0.3367, Epoch time: 0.472s.\n",
      "Epoch: 140/300, Train loss: 0.3286, Val loss: 0.3342, Min val loss: 0.3329, Min ES val loss: 0.3329, Epoch time: 0.580s.\n",
      "Epoch: 160/300, Train loss: 0.3244, Val loss: 0.3319, Min val loss: 0.3305, Min ES val loss: 0.3305, Epoch time: 0.522s.\n",
      "Epoch: 180/300, Train loss: 0.3197, Val loss: 0.3320, Min val loss: 0.3304, Min ES val loss: 0.3304, Epoch time: 0.477s.\n",
      "Epoch: 200/300, Train loss: 0.3167, Val loss: 0.3283, Min val loss: 0.3263, Min ES val loss: 0.3263, Epoch time: 0.520s.\n",
      "Epoch: 220/300, Train loss: 0.3152, Val loss: 0.3313, Min val loss: 0.3263, Min ES val loss: 0.3263, Epoch time: 0.502s.\n",
      "Epoch: 240/300, Train loss: 0.3131, Val loss: 0.3289, Min val loss: 0.3263, Min ES val loss: 0.3263, Epoch time: 0.497s.\n",
      "Epoch: 260/300, Train loss: 0.3072, Val loss: 0.3281, Min val loss: 0.3263, Min ES val loss: 0.3263, Epoch time: 0.467s.\n",
      "Epoch: 280/300, Train loss: 0.3081, Val loss: 0.3270, Min val loss: 0.3256, Min ES val loss: 0.3256, Epoch time: 0.576s.\n",
      "Training log_loss loss: 0.30177\n",
      "Validation log_loss loss: 0.32556\n",
      "Testing log_loss loss: 0.32181\n",
      "Trainer saved. To load the trainer, run trainer = load_trainer(path='/tmp/tmpqjlhi3zp/output/adult/2023-09-05-20-35-24-0_UserInputConfig/trainer.pkl')\n",
      "\n",
      "-------------TabNetFromTorch End-------------\n",
      "\n",
      "PytorchTabular metrics\n",
      "TabNet 1/1\n",
      "WideDeep metrics\n",
      "TabNet 1/1\n",
      "TabNetFromAbstract metrics\n",
      "TabNet 1/1\n",
      "TabNetFromTorch metrics\n",
      "TabNet 1/1\n",
      "Trainer saved. To load the trainer, run trainer = load_trainer(path='/tmp/tmpqjlhi3zp/output/adult/2023-09-05-20-35-24-0_UserInputConfig/trainer.pkl')\n"
     ]
    },
    {
     "data": {
      "text/plain": "              Program   Model  Training F1_SCORE  Training PRECISION_SCORE  \\\n0      PytorchTabular  TabNet           0.663081                  0.757096   \n1            WideDeep  TabNet           0.664649                  0.734311   \n2     TabNetFromTorch  TabNet           0.687457                  0.739652   \n3  TabNetFromAbstract  TabNet           0.693543                  0.759231   \n\n   Training RECALL_SCORE  Training JACCARD_SCORE  Training ACCURACY_SCORE  \\\n0               0.589836                0.495977                 0.855702   \n1               0.607059                0.497734                 0.852529   \n2               0.642143                0.523760                 0.859439   \n3               0.638316                0.530858                 0.864199   \n\n   Training BALANCED_ACCURACY_SCORE  Training COHEN_KAPPA_SCORE  \\\n0                          0.764917                    0.573066   \n1                          0.768709                    0.571219   \n2                          0.785239                    0.597370   \n3                          0.787067                    0.607153   \n\n   Training HAMMING_LOSS  ...  Validation ACCURACY_SCORE  \\\n0               0.144298  ...                   0.847359   \n1               0.147471  ...                   0.845055   \n2               0.140561  ...                   0.848434   \n3               0.135801  ...                   0.846284   \n\n   Validation BALANCED_ACCURACY_SCORE  Validation COHEN_KAPPA_SCORE  \\\n0                            0.753237                      0.548046   \n1                            0.762183                      0.552930   \n2                            0.772037                      0.567450   \n3                            0.763210                      0.555862   \n\n   Validation HAMMING_LOSS  Validation MATTHEWS_CORRCOEF  \\\n0                 0.152641                      0.555038   \n1                 0.154945                      0.556004   \n2                 0.151566                      0.569406   \n3                 0.153716                      0.559121   \n\n   Validation ZERO_ONE_LOSS  Validation ROC_AUC_SCORE  Validation LOG_LOSS  \\\n0                  0.152641                  0.895756             0.336181   \n1                  0.154945                  0.894150             0.338528   \n2                  0.151566                  0.901323             0.325557   \n3                  0.153716                  0.901369             0.328637   \n\n   Validation BRIER_SCORE_LOSS  Validation AVERAGE_PRECISION_SCORE  \n0                     0.106143                            0.855486  \n1                     0.107826                            0.852035  \n2                     0.104401                            0.864685  \n3                     0.104109                            0.860162  \n\n[4 rows x 44 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Program</th>\n      <th>Model</th>\n      <th>Training F1_SCORE</th>\n      <th>Training PRECISION_SCORE</th>\n      <th>Training RECALL_SCORE</th>\n      <th>Training JACCARD_SCORE</th>\n      <th>Training ACCURACY_SCORE</th>\n      <th>Training BALANCED_ACCURACY_SCORE</th>\n      <th>Training COHEN_KAPPA_SCORE</th>\n      <th>Training HAMMING_LOSS</th>\n      <th>...</th>\n      <th>Validation ACCURACY_SCORE</th>\n      <th>Validation BALANCED_ACCURACY_SCORE</th>\n      <th>Validation COHEN_KAPPA_SCORE</th>\n      <th>Validation HAMMING_LOSS</th>\n      <th>Validation MATTHEWS_CORRCOEF</th>\n      <th>Validation ZERO_ONE_LOSS</th>\n      <th>Validation ROC_AUC_SCORE</th>\n      <th>Validation LOG_LOSS</th>\n      <th>Validation BRIER_SCORE_LOSS</th>\n      <th>Validation AVERAGE_PRECISION_SCORE</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>PytorchTabular</td>\n      <td>TabNet</td>\n      <td>0.663081</td>\n      <td>0.757096</td>\n      <td>0.589836</td>\n      <td>0.495977</td>\n      <td>0.855702</td>\n      <td>0.764917</td>\n      <td>0.573066</td>\n      <td>0.144298</td>\n      <td>...</td>\n      <td>0.847359</td>\n      <td>0.753237</td>\n      <td>0.548046</td>\n      <td>0.152641</td>\n      <td>0.555038</td>\n      <td>0.152641</td>\n      <td>0.895756</td>\n      <td>0.336181</td>\n      <td>0.106143</td>\n      <td>0.855486</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>WideDeep</td>\n      <td>TabNet</td>\n      <td>0.664649</td>\n      <td>0.734311</td>\n      <td>0.607059</td>\n      <td>0.497734</td>\n      <td>0.852529</td>\n      <td>0.768709</td>\n      <td>0.571219</td>\n      <td>0.147471</td>\n      <td>...</td>\n      <td>0.845055</td>\n      <td>0.762183</td>\n      <td>0.552930</td>\n      <td>0.154945</td>\n      <td>0.556004</td>\n      <td>0.154945</td>\n      <td>0.894150</td>\n      <td>0.338528</td>\n      <td>0.107826</td>\n      <td>0.852035</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>TabNetFromTorch</td>\n      <td>TabNet</td>\n      <td>0.687457</td>\n      <td>0.739652</td>\n      <td>0.642143</td>\n      <td>0.523760</td>\n      <td>0.859439</td>\n      <td>0.785239</td>\n      <td>0.597370</td>\n      <td>0.140561</td>\n      <td>...</td>\n      <td>0.848434</td>\n      <td>0.772037</td>\n      <td>0.567450</td>\n      <td>0.151566</td>\n      <td>0.569406</td>\n      <td>0.151566</td>\n      <td>0.901323</td>\n      <td>0.325557</td>\n      <td>0.104401</td>\n      <td>0.864685</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>TabNetFromAbstract</td>\n      <td>TabNet</td>\n      <td>0.693543</td>\n      <td>0.759231</td>\n      <td>0.638316</td>\n      <td>0.530858</td>\n      <td>0.864199</td>\n      <td>0.787067</td>\n      <td>0.607153</td>\n      <td>0.135801</td>\n      <td>...</td>\n      <td>0.846284</td>\n      <td>0.763210</td>\n      <td>0.555862</td>\n      <td>0.153716</td>\n      <td>0.559121</td>\n      <td>0.153716</td>\n      <td>0.901369</td>\n      <td>0.328637</td>\n      <td>0.104109</td>\n      <td>0.860162</td>\n    </tr>\n  </tbody>\n</table>\n<p>4 rows  44 columns</p>\n</div>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(device=device)\n",
    "adult_columns = [\n",
    "    \"age\",\n",
    "    \"workclass\",\n",
    "    \"fnlwgt\",\n",
    "    \"education\",\n",
    "    \"education-num\",\n",
    "    \"marital-status\",\n",
    "    \"occupation\",\n",
    "    \"relationship\",\n",
    "    \"race\",\n",
    "    \"sex\",\n",
    "    \"capital-gain\",\n",
    "    \"capital-loss\",\n",
    "    \"hours-per-week\",\n",
    "    \"native-country\",\n",
    "    \"income\",\n",
    "]\n",
    "cfg = UserConfig.from_uci(\"Adult\", column_names=adult_columns, sep=\", \")\n",
    "trainer.load_config(cfg)\n",
    "trainer.load_data()\n",
    "trainer.add_modelbases(\n",
    "    [\n",
    "        PytorchTabular(trainer, model_subset=[\"TabNet\"]),\n",
    "        WideDeep(trainer, model_subset=[\"TabNet\"]),\n",
    "        TabNetFromAbstract(trainer),\n",
    "        TabNetFromTorch(trainer),\n",
    "    ]\n",
    ")\n",
    "trainer.train(stderr_to_stdout=True)\n",
    "trainer.get_leaderboard()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally the multiclass classification task:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://archive.ics.uci.edu/static/public/53/iris.zip to /tmp/tmpqjlhi3zp/data/Iris.zip\n",
      "The project will be saved to /tmp/tmpqjlhi3zp/output/iris/2023-09-05-20-46-41-0_UserInputConfig\n",
      "Dataset size: 90 30 30\n",
      "Data saved to /tmp/tmpqjlhi3zp/output/iris/2023-09-05-20-46-41-0_UserInputConfig (data.csv and tabular_data.csv).\n",
      "{'some_param': 1.1, 'program': None, 'model_subset': None, 'exclude_models': None, 'store_in_harddisk': True}\n",
      "\n",
      "-------------Run PytorchTabular-------------\n",
      "\n",
      "Training TabNet\n",
      "Global seed set to 42\n",
      "2023-09-05 20:46:41,887 - {pytorch_tabular.tabular_model:473} - INFO - Preparing the DataLoaders\n",
      "2023-09-05 20:46:41,887 - {pytorch_tabular.tabular_datamodule:290} - INFO - Setting up the datamodule for classification task\n",
      "2023-09-05 20:46:41,894 - {pytorch_tabular.tabular_model:521} - INFO - Preparing the Model: TabNetModel\n",
      "2023-09-05 20:46:41,908 - {pytorch_tabular.tabular_model:268} - INFO - Preparing the Trainer\n",
      "Auto select gpus: [0]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "2023-09-05 20:46:41,919 - {pytorch_tabular.tabular_model:582} - INFO - Training Started\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name             | Type             | Params\n",
      "------------------------------------------------------\n",
      "0 | _embedding_layer | Identity         | 0     \n",
      "1 | _backbone        | TabNetBackbone   | 5.9 K \n",
      "2 | _head            | Identity         | 0     \n",
      "3 | loss             | CrossEntropyLoss | 0     \n",
      "------------------------------------------------------\n",
      "5.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "5.9 K     Total params\n",
      "0.024     Total estimated model params size (MB)\n",
      "Epoch: 1/300, Train loss: 1.3527, Val loss: 3.6434, Min val loss: 3.6434, Epoch time: 0.053s.\n",
      "Epoch: 20/300, Train loss: 0.8093, Val loss: 2.1175, Min val loss: 2.1175, Epoch time: 0.019s.\n",
      "Epoch: 40/300, Train loss: 0.5522, Val loss: 1.1805, Min val loss: 1.1805, Epoch time: 0.019s.\n",
      "Epoch: 60/300, Train loss: 0.3886, Val loss: 0.8055, Min val loss: 0.8055, Epoch time: 0.018s.\n",
      "Epoch: 80/300, Train loss: 0.2752, Val loss: 0.6750, Min val loss: 0.6750, Epoch time: 0.027s.\n",
      "Epoch: 100/300, Train loss: 0.2040, Val loss: 0.6147, Min val loss: 0.6147, Epoch time: 0.019s.\n",
      "Epoch: 120/300, Train loss: 0.1545, Val loss: 0.5562, Min val loss: 0.5562, Epoch time: 0.019s.\n",
      "Epoch: 140/300, Train loss: 0.1169, Val loss: 0.4918, Min val loss: 0.4918, Epoch time: 0.020s.\n",
      "Epoch: 160/300, Train loss: 0.0898, Val loss: 0.4398, Min val loss: 0.4398, Epoch time: 0.018s.\n",
      "Epoch: 180/300, Train loss: 0.0706, Val loss: 0.4000, Min val loss: 0.4000, Epoch time: 0.018s.\n",
      "Epoch: 200/300, Train loss: 0.0564, Val loss: 0.3694, Min val loss: 0.3694, Epoch time: 0.018s.\n",
      "Epoch: 220/300, Train loss: 0.0457, Val loss: 0.3455, Min val loss: 0.3455, Epoch time: 0.018s.\n",
      "Epoch: 240/300, Train loss: 0.0375, Val loss: 0.3314, Min val loss: 0.3314, Epoch time: 0.018s.\n",
      "Epoch: 260/300, Train loss: 0.0311, Val loss: 0.3249, Min val loss: 0.3249, Epoch time: 0.018s.\n",
      "Epoch: 280/300, Train loss: 0.0263, Val loss: 0.3223, Min val loss: 0.3221, Epoch time: 0.018s.\n",
      "Epoch: 300/300, Train loss: 0.0224, Val loss: 0.3200, Min val loss: 0.3193, Epoch time: 0.018s.\n",
      "`Trainer.fit` stopped: `max_epochs=300` reached.\n",
      "2023-09-05 20:46:51,646 - {pytorch_tabular.tabular_model:584} - INFO - Training the model completed\n",
      "2023-09-05 20:46:51,646 - {pytorch_tabular.tabular_model:1258} - INFO - Loading the best model\n",
      "/home/xlluo/anaconda3/envs/mlfatigue/lib/python3.8/site-packages/pytorch_lightning/utilities/cloud_io.py:33: LightningDeprecationWarning: `pytorch_lightning.utilities.cloud_io.get_filesystem` has been deprecated in v1.8.0 and will be removed in v1.10.0. Please use `lightning_lite.utilities.cloud_io.get_filesystem` instead.\n",
      "  rank_zero_deprecation(\n",
      "Training log_loss loss: 0.03003\n",
      "Validation log_loss loss: 0.31931\n",
      "Testing log_loss loss: 0.09626\n",
      "Trainer saved. To load the trainer, run trainer = load_trainer(path='/tmp/tmpqjlhi3zp/output/iris/2023-09-05-20-46-41-0_UserInputConfig/trainer.pkl')\n",
      "\n",
      "-------------PytorchTabular End-------------\n",
      "\n",
      "\n",
      "-------------Run WideDeep-------------\n",
      "\n",
      "Training TabNet\n",
      "Epoch: 1/300, Train loss: 1.3533, Val loss: 3.6438, Min val loss: 3.6438\n",
      "Epoch: 21/300, Train loss: 0.7912, Val loss: 2.0427, Min val loss: 2.0427\n",
      "Epoch: 41/300, Train loss: 0.5492, Val loss: 1.1558, Min val loss: 1.1558\n",
      "Epoch: 61/300, Train loss: 0.3922, Val loss: 0.8101, Min val loss: 0.8101\n",
      "Epoch: 81/300, Train loss: 0.2786, Val loss: 0.6720, Min val loss: 0.6720\n",
      "Epoch: 101/300, Train loss: 0.2061, Val loss: 0.6045, Min val loss: 0.6045\n",
      "Epoch: 121/300, Train loss: 0.1563, Val loss: 0.5392, Min val loss: 0.5392\n",
      "Epoch: 141/300, Train loss: 0.1187, Val loss: 0.4764, Min val loss: 0.4764\n",
      "Epoch: 161/300, Train loss: 0.0919, Val loss: 0.4189, Min val loss: 0.4189\n",
      "Epoch: 181/300, Train loss: 0.0722, Val loss: 0.3798, Min val loss: 0.3798\n",
      "Epoch: 201/300, Train loss: 0.0577, Val loss: 0.3444, Min val loss: 0.3444\n",
      "Epoch: 221/300, Train loss: 0.0468, Val loss: 0.3301, Min val loss: 0.3301\n",
      "Epoch: 241/300, Train loss: 0.0387, Val loss: 0.3202, Min val loss: 0.3202\n",
      "Epoch: 261/300, Train loss: 0.0324, Val loss: 0.3105, Min val loss: 0.3105\n",
      "Epoch: 281/300, Train loss: 0.0276, Val loss: 0.3076, Min val loss: 0.3075\n",
      "Restoring model weights from the end of the best epoch\n",
      "Training log_loss loss: 0.03150\n",
      "Validation log_loss loss: 0.30604\n",
      "Testing log_loss loss: 0.13717\n",
      "Trainer saved. To load the trainer, run trainer = load_trainer(path='/tmp/tmpqjlhi3zp/output/iris/2023-09-05-20-46-41-0_UserInputConfig/trainer.pkl')\n",
      "\n",
      "-------------WideDeep End-------------\n",
      "\n",
      "\n",
      "-------------Run TabNetFromAbstract-------------\n",
      "\n",
      "Training TabNet\n",
      "/home/xlluo/anaconda3/envs/mlfatigue/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "epoch 0  | loss: 1.56647 | val_0_logloss: 2.77843 |  0:00:00s\n",
      "epoch 20 | loss: 0.84741 | val_0_logloss: 1.56011 |  0:00:00s\n",
      "epoch 40 | loss: 0.57801 | val_0_logloss: 1.30488 |  0:00:00s\n",
      "epoch 60 | loss: 0.42034 | val_0_logloss: 0.94963 |  0:00:01s\n",
      "epoch 80 | loss: 0.31006 | val_0_logloss: 0.81757 |  0:00:01s\n",
      "epoch 100| loss: 0.23076 | val_0_logloss: 0.67336 |  0:00:01s\n",
      "epoch 120| loss: 0.17021 | val_0_logloss: 0.565   |  0:00:02s\n",
      "epoch 140| loss: 0.12282 | val_0_logloss: 0.47427 |  0:00:02s\n",
      "epoch 160| loss: 0.08769 | val_0_logloss: 0.39075 |  0:00:02s\n",
      "epoch 180| loss: 0.06413 | val_0_logloss: 0.35032 |  0:00:03s\n",
      "epoch 200| loss: 0.04945 | val_0_logloss: 0.34441 |  0:00:03s\n",
      "epoch 220| loss: 0.03516 | val_0_logloss: 0.31676 |  0:00:04s\n",
      "epoch 240| loss: 0.02685 | val_0_logloss: 0.30498 |  0:00:04s\n",
      "epoch 260| loss: 0.02127 | val_0_logloss: 0.30292 |  0:00:04s\n",
      "epoch 280| loss: 0.01733 | val_0_logloss: 0.30212 |  0:00:05s\n",
      "Stop training because you reached max_epochs = 300 with best_epoch = 298 and best_val_0_logloss = 0.29842\n",
      "/home/xlluo/anaconda3/envs/mlfatigue/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "Training log_loss loss: 0.02804\n",
      "Validation log_loss loss: 0.29842\n",
      "Testing log_loss loss: 0.06466\n",
      "Trainer saved. To load the trainer, run trainer = load_trainer(path='/tmp/tmpqjlhi3zp/output/iris/2023-09-05-20-46-41-0_UserInputConfig/trainer.pkl')\n",
      "\n",
      "-------------TabNetFromAbstract End-------------\n",
      "\n",
      "\n",
      "-------------Run TabNetFromTorch-------------\n",
      "\n",
      "Training TabNet\n",
      "Auto select gpus: [0]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                | Type             | Params\n",
      "---------------------------------------------------------\n",
      "0 | default_loss_fn     | CrossEntropyLoss | 0     \n",
      "1 | default_output_norm | Softmax          | 0     \n",
      "2 | network             | TabNet           | 5.9 K \n",
      "---------------------------------------------------------\n",
      "5.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "5.9 K     Total params\n",
      "0.024     Total estimated model params size (MB)\n",
      "Epoch: 1/300, Train loss: 1.3527, Val loss: 3.6726, Min val loss: 3.6726, Min ES val loss: 3.6726, Epoch time: 0.018s.\n",
      "Epoch: 20/300, Train loss: 0.8086, Val loss: 2.0715, Min val loss: 2.0715, Min ES val loss: 2.0715, Epoch time: 0.016s.\n",
      "Epoch: 40/300, Train loss: 0.5587, Val loss: 1.1550, Min val loss: 1.1550, Min ES val loss: 1.1550, Epoch time: 0.016s.\n",
      "Epoch: 60/300, Train loss: 0.3997, Val loss: 0.8180, Min val loss: 0.8180, Min ES val loss: 0.8180, Epoch time: 0.016s.\n",
      "Epoch: 80/300, Train loss: 0.2834, Val loss: 0.6803, Min val loss: 0.6803, Min ES val loss: 0.6803, Epoch time: 0.016s.\n",
      "Epoch: 100/300, Train loss: 0.2105, Val loss: 0.6035, Min val loss: 0.6035, Min ES val loss: 0.6035, Epoch time: 0.016s.\n",
      "Epoch: 120/300, Train loss: 0.1592, Val loss: 0.5262, Min val loss: 0.5262, Min ES val loss: 0.5262, Epoch time: 0.017s.\n",
      "Epoch: 140/300, Train loss: 0.1206, Val loss: 0.4571, Min val loss: 0.4571, Min ES val loss: 0.4571, Epoch time: 0.017s.\n",
      "Epoch: 160/300, Train loss: 0.0924, Val loss: 0.4145, Min val loss: 0.4145, Min ES val loss: 0.4145, Epoch time: 0.017s.\n",
      "Epoch: 180/300, Train loss: 0.0721, Val loss: 0.3756, Min val loss: 0.3756, Min ES val loss: 0.3756, Epoch time: 0.042s.\n",
      "Epoch: 200/300, Train loss: 0.0574, Val loss: 0.3437, Min val loss: 0.3437, Min ES val loss: 0.3437, Epoch time: 0.017s.\n",
      "Epoch: 220/300, Train loss: 0.0465, Val loss: 0.3267, Min val loss: 0.3267, Min ES val loss: 0.3267, Epoch time: 0.017s.\n",
      "Epoch: 240/300, Train loss: 0.0384, Val loss: 0.3145, Min val loss: 0.3145, Min ES val loss: 0.3145, Epoch time: 0.016s.\n",
      "Epoch: 260/300, Train loss: 0.0321, Val loss: 0.3052, Min val loss: 0.3052, Min ES val loss: 0.3052, Epoch time: 0.018s.\n",
      "Epoch: 280/300, Train loss: 0.0272, Val loss: 0.3027, Min val loss: 0.3027, Min ES val loss: 0.3027, Epoch time: 0.025s.\n",
      "Epoch: 300/300, Train loss: 0.0233, Val loss: 0.3008, Min val loss: 0.3008, Min ES val loss: 0.3008, Epoch time: 0.016s.\n",
      "`Trainer.fit` stopped: `max_epochs=300` reached.\n",
      "Training log_loss loss: 0.03055\n",
      "Validation log_loss loss: 0.30076\n",
      "Testing log_loss loss: 0.10753\n",
      "Trainer saved. To load the trainer, run trainer = load_trainer(path='/tmp/tmpqjlhi3zp/output/iris/2023-09-05-20-46-41-0_UserInputConfig/trainer.pkl')\n",
      "\n",
      "-------------TabNetFromTorch End-------------\n",
      "\n",
      "PytorchTabular metrics\n",
      "TabNet 1/1\n",
      "WideDeep metrics\n",
      "TabNet 1/1\n",
      "TabNetFromAbstract metrics\n",
      "TabNet 1/1\n",
      "TabNetFromTorch metrics\n",
      "TabNet 1/1\n",
      "Trainer saved. To load the trainer, run trainer = load_trainer(path='/tmp/tmpqjlhi3zp/output/iris/2023-09-05-20-46-41-0_UserInputConfig/trainer.pkl')\n"
     ]
    },
    {
     "data": {
      "text/plain": "              Program   Model  Training ACCURACY_SCORE  \\\n0            WideDeep  TabNet                      1.0   \n1      PytorchTabular  TabNet                      1.0   \n2  TabNetFromAbstract  TabNet                      1.0   \n3     TabNetFromTorch  TabNet                      1.0   \n\n   Training BALANCED_ACCURACY_SCORE  Training COHEN_KAPPA_SCORE  \\\n0                               1.0                         1.0   \n1                               1.0                         1.0   \n2                               1.0                         1.0   \n3                               1.0                         1.0   \n\n   Training HAMMING_LOSS  Training MATTHEWS_CORRCOEF  Training ZERO_ONE_LOSS  \\\n0                    0.0                         1.0                     0.0   \n1                    0.0                         1.0                     0.0   \n2                    0.0                         1.0                     0.0   \n3                    0.0                         1.0                     0.0   \n\n   Training PRECISION_SCORE_MACRO  Training PRECISION_SCORE_MICRO  ...  \\\n0                             1.0                             1.0  ...   \n1                             1.0                             1.0  ...   \n2                             1.0                             1.0  ...   \n3                             1.0                             1.0  ...   \n\n   Validation F1_SCORE_MICRO  Validation F1_SCORE_WEIGHTED  \\\n0                   0.933333                      0.934656   \n1                   0.933333                      0.934656   \n2                   0.866667                      0.866667   \n3                   0.933333                      0.934656   \n\n   Validation JACCARD_SCORE_MACRO  Validation JACCARD_SCORE_MICRO  \\\n0                        0.888889                        0.875000   \n1                        0.888889                        0.875000   \n2                        0.788235                        0.764706   \n3                        0.888889                        0.875000   \n\n   Validation JACCARD_SCORE_WEIGHTED  Validation TOP_K_ACCURACY_SCORE  \\\n0                           0.880000                              1.0   \n1                           0.880000                              1.0   \n2                           0.775686                              1.0   \n3                           0.880000                              1.0   \n\n   Validation LOG_LOSS  Validation ROC_AUC_SCORE_OVR_MACRO  \\\n0             0.306037                            0.980985   \n1             0.319313                            0.971684   \n2             0.298424                            0.972997   \n3             0.300758                            0.980985   \n\n   Validation ROC_AUC_SCORE_OVR_WEIGHTED  Validation ROC_AUC_SCORE_OVO  \n0                               0.975455                      0.979940  \n1                               0.962828                      0.969762  \n2                               0.970101                      0.973661  \n3                               0.975455                      0.979940  \n\n[4 rows x 71 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Program</th>\n      <th>Model</th>\n      <th>Training ACCURACY_SCORE</th>\n      <th>Training BALANCED_ACCURACY_SCORE</th>\n      <th>Training COHEN_KAPPA_SCORE</th>\n      <th>Training HAMMING_LOSS</th>\n      <th>Training MATTHEWS_CORRCOEF</th>\n      <th>Training ZERO_ONE_LOSS</th>\n      <th>Training PRECISION_SCORE_MACRO</th>\n      <th>Training PRECISION_SCORE_MICRO</th>\n      <th>...</th>\n      <th>Validation F1_SCORE_MICRO</th>\n      <th>Validation F1_SCORE_WEIGHTED</th>\n      <th>Validation JACCARD_SCORE_MACRO</th>\n      <th>Validation JACCARD_SCORE_MICRO</th>\n      <th>Validation JACCARD_SCORE_WEIGHTED</th>\n      <th>Validation TOP_K_ACCURACY_SCORE</th>\n      <th>Validation LOG_LOSS</th>\n      <th>Validation ROC_AUC_SCORE_OVR_MACRO</th>\n      <th>Validation ROC_AUC_SCORE_OVR_WEIGHTED</th>\n      <th>Validation ROC_AUC_SCORE_OVO</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>WideDeep</td>\n      <td>TabNet</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>...</td>\n      <td>0.933333</td>\n      <td>0.934656</td>\n      <td>0.888889</td>\n      <td>0.875000</td>\n      <td>0.880000</td>\n      <td>1.0</td>\n      <td>0.306037</td>\n      <td>0.980985</td>\n      <td>0.975455</td>\n      <td>0.979940</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>PytorchTabular</td>\n      <td>TabNet</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>...</td>\n      <td>0.933333</td>\n      <td>0.934656</td>\n      <td>0.888889</td>\n      <td>0.875000</td>\n      <td>0.880000</td>\n      <td>1.0</td>\n      <td>0.319313</td>\n      <td>0.971684</td>\n      <td>0.962828</td>\n      <td>0.969762</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>TabNetFromAbstract</td>\n      <td>TabNet</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>...</td>\n      <td>0.866667</td>\n      <td>0.866667</td>\n      <td>0.788235</td>\n      <td>0.764706</td>\n      <td>0.775686</td>\n      <td>1.0</td>\n      <td>0.298424</td>\n      <td>0.972997</td>\n      <td>0.970101</td>\n      <td>0.973661</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>TabNetFromTorch</td>\n      <td>TabNet</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>...</td>\n      <td>0.933333</td>\n      <td>0.934656</td>\n      <td>0.888889</td>\n      <td>0.875000</td>\n      <td>0.880000</td>\n      <td>1.0</td>\n      <td>0.300758</td>\n      <td>0.980985</td>\n      <td>0.975455</td>\n      <td>0.979940</td>\n    </tr>\n  </tbody>\n</table>\n<p>4 rows  71 columns</p>\n</div>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(device=device)\n",
    "iris_columns = [\n",
    "    \"sepal length\",\n",
    "    \"sepal width\",\n",
    "    \"petal length\",\n",
    "    \"petal width\",\n",
    "    \"class\",\n",
    "]\n",
    "cfg = UserConfig.from_uci(\"Iris\", column_names=iris_columns, datafile_name=\"iris\")\n",
    "trainer.load_config(cfg)\n",
    "trainer.load_data()\n",
    "trainer.add_modelbases(\n",
    "    [\n",
    "        PytorchTabular(trainer, model_subset=[\"TabNet\"]),\n",
    "        WideDeep(trainer, model_subset=[\"TabNet\"]),\n",
    "        TabNetFromAbstract(trainer),\n",
    "        TabNetFromTorch(trainer),\n",
    "    ]\n",
    ")\n",
    "trainer.train(stderr_to_stdout=True)\n",
    "trainer.get_leaderboard()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Results show that models perform much worse on the validation set than on the testing set. To get reliable results, we recommend using cross-validation to get the leaderboard:\n",
    "\n",
    "```python\n",
    "# trainer.train(stderr_to_stdout=True)  # No need to run `train`\n",
    "trainer.get_leaderboard(cross_validation=5, split_type=\"cv\")\n",
    "```"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}